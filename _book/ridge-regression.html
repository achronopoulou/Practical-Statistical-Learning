<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.4 Ridge Regression | CS 598 Practical Statistical Learning</title>
  <meta name="description" content="Lecture notes for “CS 598 PSL”." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="3.4 Ridge Regression | CS 598 Practical Statistical Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for “CS 598 PSL”." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.4 Ridge Regression | CS 598 Practical Statistical Learning" />
  
  <meta name="twitter:description" content="Lecture notes for “CS 598 PSL”." />
  

<meta name="author" content="Alexandra Chronopoulou" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="shrinkage-methods.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">CS 598 PSL A. Chronopoulou</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><font color="darkblue"> Course Information </font></a></li>
<li class="chapter" data-level="1" data-path="introduction-to-statistical-learning.html"><a href="introduction-to-statistical-learning.html"><i class="fa fa-check"></i><b>1</b> Introduction to Statistical Learning</a>
<ul>
<li class="chapter" data-level="1.1" data-path="examples-of-statistical-learning-problems.html"><a href="examples-of-statistical-learning-problems.html"><i class="fa fa-check"></i><b>1.1</b> Examples of Statistical Learning Problems</a></li>
<li class="chapter" data-level="1.2" data-path="supervised-learning-framework.html"><a href="supervised-learning-framework.html"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning Framework</a></li>
<li class="chapter" data-level="1.3" data-path="why-is-statistical-learning-is-challenging.html"><a href="why-is-statistical-learning-is-challenging.html"><i class="fa fa-check"></i><b>1.3</b> Why is Statistical Learning is Challenging</a></li>
<li class="chapter" data-level="1.4" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>1.4</b> Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="1.5" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html"><i class="fa fa-check"></i><b>1.5</b> Two Toy Examples: <span class="math inline">\(k\)</span>NN vs. Linear Regression</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>1.5.1</b> <span class="math inline">\(k\)</span>-Nearest Neighbors</a></li>
<li class="chapter" data-level="1.5.2" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html#linear-regression"><i class="fa fa-check"></i><b>1.5.2</b> Linear Regression</a></li>
<li class="chapter" data-level="1.5.3" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html#simulated-binary-classification-example"><i class="fa fa-check"></i><b>1.5.3</b> Simulated Binary Classification Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression-models.html"><a href="linear-regression-models.html"><i class="fa fa-check"></i><b>2</b> Linear Regression Models</a>
<ul>
<li class="chapter" data-level="2.1" data-path="multiple-linear-regression-mlr-model.html"><a href="multiple-linear-regression-mlr-model.html"><i class="fa fa-check"></i><b>2.1</b> Multiple Linear Regression (MLR) Model</a></li>
<li class="chapter" data-level="2.2" data-path="mlr-model-fitting.html"><a href="mlr-model-fitting.html"><i class="fa fa-check"></i><b>2.2</b> MLR Model Fitting</a></li>
<li class="chapter" data-level="2.3" data-path="least-squares-normal-equations.html"><a href="least-squares-normal-equations.html"><i class="fa fa-check"></i><b>2.3</b> Least-Squares &amp; Normal Equations</a></li>
<li class="chapter" data-level="2.4" data-path="goodness-of-fit-r-square.html"><a href="goodness-of-fit-r-square.html"><i class="fa fa-check"></i><b>2.4</b> Goodness-Of-Fit: <span class="math inline">\(R\)</span>-Square</a></li>
<li class="chapter" data-level="2.5" data-path="linear-transformations-on-x.html"><a href="linear-transformations-on-x.html"><i class="fa fa-check"></i><b>2.5</b> Linear Transformations on <span class="math inline">\(X\)</span></a></li>
<li class="chapter" data-level="2.6" data-path="rank-deficiency.html"><a href="rank-deficiency.html"><i class="fa fa-check"></i><b>2.6</b> Rank deficiency</a></li>
<li class="chapter" data-level="2.7" data-path="hypothesis-testing-in-mlr.html"><a href="hypothesis-testing-in-mlr.html"><i class="fa fa-check"></i><b>2.7</b> Hypothesis Testing in MLR</a></li>
<li class="chapter" data-level="2.8" data-path="categorical-variables-in-mlr.html"><a href="categorical-variables-in-mlr.html"><i class="fa fa-check"></i><b>2.8</b> Categorical Variables in MLR</a></li>
<li class="chapter" data-level="2.9" data-path="collinearity.html"><a href="collinearity.html"><i class="fa fa-check"></i><b>2.9</b> Collinearity</a></li>
<li class="chapter" data-level="2.10" data-path="model-diagnostics.html"><a href="model-diagnostics.html"><i class="fa fa-check"></i><b>2.10</b> Model Diagnostics</a></li>
<li class="chapter" data-level="2.11" data-path="the-birthweight-example-in-r.html"><a href="the-birthweight-example-in-r.html"><i class="fa fa-check"></i><b>2.11</b> The <code>Birthweight</code> Example in <code>R</code></a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="variable-selection-regularization.html"><a href="variable-selection-regularization.html"><i class="fa fa-check"></i><b>3</b> Variable Selection &amp; Regularization</a>
<ul>
<li class="chapter" data-level="3.1" data-path="training-vs.-testing-errors.html"><a href="training-vs.-testing-errors.html"><i class="fa fa-check"></i><b>3.1</b> Training vs. Testing Errors</a></li>
<li class="chapter" data-level="3.2" data-path="subset-selection.html"><a href="subset-selection.html"><i class="fa fa-check"></i><b>3.2</b> Subset Selection</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="subset-selection.html"><a href="subset-selection.html#information-criteria-based-procedures"><i class="fa fa-check"></i><b>3.2.1</b> Information Criteria-based procedures</a></li>
<li class="chapter" data-level="3.2.2" data-path="subset-selection.html"><a href="subset-selection.html#searching-algorithms"><i class="fa fa-check"></i><b>3.2.2</b> Searching Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html"><i class="fa fa-check"></i><b>3.3</b> Shrinkage Methods</a></li>
<li class="chapter" data-level="3.4" data-path="ridge-regression.html"><a href="ridge-regression.html"><i class="fa fa-check"></i><b>3.4</b> Ridge Regression</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">CS 598 Practical Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="ridge-regression" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Ridge Regression<a href="ridge-regression.html#ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Ridge regression assumes that after normalization, some of the regression coefficients should not be very large. Ridge regression is very useful when you have collinearity and the LS regression coefficients are unstable.</p>
<p><br></p>
<div class="motivationbox">
<p><strong>Ridge Regression</strong></p>
<p>The idea of the method is to use a penalized regression by adding a <strong>penalty</strong> term to the LS minimization problem :
<span class="math display">\[\text{minimize }\,  (y-X \beta)^T(y-X \beta) + \lambda \sum_j \beta_j^2\]</span>
for some <span class="math inline">\(\lambda \ge 0\)</span>. The penalty term is <span class="math inline">\(\sum_j \beta_j^2\)</span>.</p>
</div>
<p>For the method to be more effective, we prefer to <em>standardize</em> the predictors first (centered by their means and scaled by their standard deviations) and center the response <span class="math inline">\(y\)</span> as well. A big benefit of the ridge regression is that we can easily obtain closed-form solutions for the <span class="math inline">\(\beta\)</span> coefficients. Indeed, solving the minimization problem we get:
<font color=Darkblue>
<span class="math display">\[\hat{\beta}_{\text{Ridge}}=(X^T X+\underbrace{\lambda I}_{\text{ridge}})^{-1} X^T y\]</span>
</font>
Note that the extra term <span class="math inline">\(\lambda I\)</span> or <strong>ridge</strong> in the <span class="math inline">\(X^\top X\)</span> matrix.</p>
<p>Note that when <span class="math inline">\(\lambda=0\)</span> the ridge regression estimation problem reduces to the standard least squares problem, while when <span class="math inline">\(\lambda \rightarrow \infty\)</span>, <span class="math inline">\(\hat{\beta} \rightarrow \mathbf{0}\)</span>. The value of <span class="math inline">\(\lambda\)</span> can be also chosen using automated methods such as <em>Generalized Cross-Validation</em> (GCV) (similar to Cross-Validation). The main <em>disadvantage</em> of the ridge regression estimators is that they are <strong>biased</strong>.</p>
<p><br><br></p>
<p>Ridge was initially introduced to address multicollinearity issues, by adding a non-negative constant to the diagonal of the design matrix.</p>
<div id="ridge-regression-ls-coefficients" class="section level4 unnumbered hasAnchor">
<h4>Ridge Regression LS Coefficients<a href="ridge-regression.html#ridge-regression-ls-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>If we assume that <span class="math inline">\(\mathbf{X}^T\mathbf{X} = \mathbf{I}_p\)</span>, that is the columns of the design matrix are orthogonal:</p>
<p><img src="images/week3/RL_comp1.png" width="75%" style="display: block; margin: auto;" /></p>
<p>If the columns of the design matrix are not orthogonal, then we can run the regression against an orthonormal version of <span class="math inline">\(\mathbf{X}\)</span>, known as principal components analysis}, or singular value decomposition}.</p>
<p>Singular Value Decomposition (SVD)</p>
<p>SVD of <span class="math inline">\(\mathbf{X}_{n\times p}\)</span></p>
<p><span class="math display">\[\mathbf{X} = \mathbf{U}_{n\times p} \; \mathbf{D}_{p\times p}\, \mathbf{V}_{p\times p}^T\]</span></p>
<p><span class="math inline">\(\mathbf{U}_{n\times p}\)</span> columns are spanning the column space of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p><span class="math inline">\(\mathbf{V}_{p\times p}\)</span> spanning the row space of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p><span class="math inline">\(\mathbf{D}_{p\times p}\)</span> diagonal values with <span class="math inline">\(d_1 \geq \ldots \geq d_p \geq 0\)</span> the singular values of <span class="math inline">\(\mathbf{X}\)</span>. If one or more <span class="math inline">\(d_j = 0\)</span>, then <span class="math inline">\(\mathbf{X}\)</span> is singular.</p>
<p>Singular Value Decomposition (SVD)</p>
<p>SVD of Fitted Values: LS} vs. Ridge</p>
<p>The ridge estimate <span class="math inline">\(\hat{\beta}_{ridge}\)</span> shrinks the LS estimate <span class="math inline">\(\hat{\beta}_{LS}\)</span> by a factor of <span class="math inline">\(\frac{d_j^2}{d_j^2 + \lambda}\)</span>}, and the smaller <span class="math inline">\(d_j^2\)</span> the greater the shrinkage.
<span class="math inline">\(\longrightarrow\)</span> the smaller the eigenvalues the more the shrinkage.</p>
<p>Understanding the Shrinkage</p>
<p>Ridge regression computes the coordinates of <span class="math inline">\(\mathbf{y}\)</span> with respect to an orthonormal basis <span class="math inline">\(\mathbf{U}\)</span>.</p>
<p>It shrinks the coordinates by the factors <span class="math inline">\(\frac{d_j^2}{d_j^2 + \lambda\)</span>, and the smaller <span class="math inline">\(d_j^2\)</span> the greater the shrinkage.</p>
<p>The SVD of <span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span> can be written as
<span class="math display">\[\mathbf{X}^T \mathbf{X} =  \bigl(\mathbf{U}\; \mathbf{D}\;\mathbf{V}\bigr)^T\mathbf{U}\; \mathbf{D}\;\mathbf{V} =  \mathbf{V} \; \mathbf{D}^2\; \mathbf{V}^T\]</span>
which is the eigen-decomposition of <span class="math inline">\(\mathbf{X}^T \mathbf{X}\)</span>.</p>
<p>The eigenvectors <span class="math inline">\(v_j\)</span>} are also called the principal components directions} of the columns of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>The first principal component} direction <span class="math inline">\(v_1\)</span> has the property that <span class="math inline">\(z_1 = \mathbf{X} v_1 = \mathbf{u}_1 d_1\)</span> has the largest sample variance} among all normalized linear combinations of the columns of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>Ridge regression projects <span class="math inline">\(\mathbf{y}\)</span> onto the principal components, and then shrinks the coefficients of the low–variance components more than the high-variance components.</p>
<p>Degrees of Freedom of a Regression Model</p>
<p>Computing the Degrees of Freedom
<span class="math display">\[df = \sum_{i=1}^{n} Cor \bigl(y_i,\; \hat{y}_i \bigr)\]</span></p>
<p>In linear regression : <span class="math inline">\(\hat{\mathbf{y}} = \mathbf{H} \mathbf{y}\)</span>, in which case
<span class="math display">\[df = \sum_{i=1}^{n} Cor \bigl(y_i,\; \hat{y}_i \bigr) = \sum_{i=1}^{n} H_{ii} = tr \bigl( \mathbf{H} \bigr) = p\]</span>
where (recall) <span class="math inline">\(\mathbf{H}\)</span> is the hat matrix equal to <span class="math inline">\(\mathbf{X} \bigl( \mathbf{X}^T \mathbf{X} \bigr)^{-1} \mathbf{X}^T \mathbf{y}\)</span>.</p>
<p>In ridge regression: <span class="math inline">\(\hat{y} = \underbrace{ \mathbf{X} \bigl(\mathbf{X}^T \mathbf{X} + \lambda \mathbf{I} \bigr)^{-1} \mathbf{X}^T}_{:= \mathbf{S}_{\lambda}} \mathbf{y}\)</span>. So, the effective df of ridge</p>
<p><span class="math display">\[df(\lambda) = tr(\mathbf{S}_{\lambda}) = tr\Biggl( \sum_{j=1}^{p} \frac{d_j^2}{d_j^2 + \lambda} \mathbf{u}_i \mathbf{u}_i^T\Biggr) =  \sum_{j=1}^{p} \frac{d_j^2}{d_j^2 + \lambda}   \]</span></p>
<p>Complexity of Ridge Regression</p>
<p>Note that when <span class="math inline">\(\lambda=0\)</span> the ridge regression estimation problem reduces to the standard LS problem.</p>
<p>When <span class="math inline">\(\lambda \rightarrow \infty\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(\boldsymbol{\hat{\beta}} = \Bigl(\mathbf{X}^T \mathbf{X} + \underbrace{\lambda I}_{\text{ridge}} \Bigr)^{-1} \mathbf{X}^T \mathbf{y} \rightarrow \mathbf{0}\)</span>.</p>
<p>Although <span class="math inline">\(\hat{\beta}_{ridge}\)</span> is <span class="math inline">\(p\)</span>-dimensional, the ridge regression doesn’t seem to use the full strength of the <span class="math inline">\(p\)</span> covariates due to the shrinkage.</p>
<p>The DF of the ridge regression should be some continuous} number between 0 and <span class="math inline">\(p\)</span>, and is decreasing with respect to <span class="math inline">\(\lambda\)</span>.</p>
<p>Ridge regression coefficient estimates are biased}, the shrinkage can help reduce the variance, which could lead to an overall smaller <span class="math inline">\(MSE\)</span>.</p>
<p>LASSO Regression}</p>
<p>In this case the estimated <span class="math inline">\(\hat{\boldsymbol{\beta}}\)</span> minimizes:
<span class="math display">\[\mbox{minimize } (y-X\boldsymbol{\beta})^\top(y-X\boldsymbol{\beta}) +\lambda \sum_j |\beta_j|\]</span>
for some <span class="math inline">\(\lambda \ge 0\)</span>.
The penalty term is <span class="math inline">\(\sum_j |\beta_j|\)</span> (<span class="math inline">\(L_1\)</span> constraint).</p>
<p>In two-dimensions the constraint defines a square}. In higher dimensions it defines a polytope.</p>
<p>LASSO is useful when the response can be explained by few predictors with zero effect on the remaining predictors (LASSO is similar to a variable selection method).</p>
<p>When <span class="math inline">\(\beta_j=0\)</span> the corresponding predictor is eliminated. This is not the case for ridge regression.</p>
<p>Obtaining the LASSO Solution</p>
<p>The LASSO solution is defined as
<span class="math display">\[\hat{\beta}_{lasso} = \arg \min_{\beta\in \mathbb{R}^{p}} \Bigl((y-X\boldsymbol{\beta})^\top(y-X\boldsymbol{\beta}) + \lambda \sum_j |\beta_j| \Bigr)\]</span></p>
<p>If we assume that <span class="math inline">\(\mathbf{X}^T\mathbf{X} = \mathbf{I}_p\)</span>}, then
<span class="math display">\[\begin{align*}
||\mathbf{y} - \mathbf{X}\beta||^2 &amp;= ||\mathbf{y} - \mathbf{X}\hat{\beta}_{LS} + \mathbf{X}\hat{\beta}_{LS} - \mathbf{X}\beta||^2\\
&amp;=||\mathbf{y} - \mathbf{X}\hat{\beta}_{LS}||^2 + ||\mathbf{X}\hat{\beta}_{LS} - \mathbf{X}\beta||^2
\end{align*}\]</span>
where
<span class="math display">\[2  \bigl( \mathbf{y} - \mathbf{X}\hat{\beta}_{LS}\bigr)^T \bigl(  \mathbf{X}\hat{\beta}_{LS} - \mathbf{X}\beta \bigr) = 2 \; r^T \; \bigl(  \mathbf{X}\hat{\beta}_{LS} - \mathbf{X} \beta\bigr) =0\]</span>
%since then-dim vector in red (which is a linear combination of columns of <span class="math inline">\(\mathbf{X}\)</span> no matter what value <span class="math inline">\(\beta\)</span> takes) is in <span class="math inline">\(\mathcal{C}(\mathbf{X})\)</span>, therefore orthogonal to t he residual vector <span class="math inline">\(r\)</span>.</p>
<p>Obtaining the LASSO Solution</p>
<p><span class="math display">\[\begin{align*}
\hat{\beta}_{lasso} &amp;= \arg \min_{\beta\in \mathbb{R}^{p}} \Bigl( ||\mathbf{y}-\mathbf{X} \beta||^2 + \lambda |\beta| \Bigr)\\
&amp;= \arg \min_{\beta\in \mathbb{R}^{p}} \Bigl( ||\mathbf{X} \hat{\beta}_{ls} -\mathbf{X} \beta||^2 + \lambda |\beta| \Bigr)\\
&amp;= \arg \min_{\beta\in \mathbb{R}^{p}} \Bigl( \bigl(  \hat{\beta}_{LS} - \beta \bigr)^T \mathbf{X}^T \mathbf{X}  \bigl(  \hat{\beta}_{LS} - \beta \bigr) + \lambda |\beta| \Bigr)\\
&amp;= \arg \min_{\beta\in \mathbb{R}^{p}} \Bigl( \bigl(  \hat{\beta}_{LS} - \beta \bigr)^T  \bigl(  \hat{\beta}_{LS} - \beta \bigr) + \lambda |\beta| \Bigr)\\
&amp;= \arg \min_{\beta_1, \ldots, \beta_p} \sum_{i=1}^{p}  \Bigl( \bigl(  \beta_{j} - \hat{\beta}_{j}^{ls} \bigr)^2  + \lambda |\beta_j| \Bigr)\\
\end{align*}\]</span>
So, we can solve the optimal <span class="math inline">\(\beta_j\)</span> for each of <span class="math inline">\(j=1, \ldots, p\)</span> separately } by solving the following generic problem:
<span class="math display">\[\arg \min_{x} \bigl( x-a \bigr )^2 + \lambda |x|,\,\, \lambda &gt;0\]</span></p>
<p>How to solve an one-dim LASSO?
Define <span class="math display">\[f(x)  = \bigl( x-a \bigr )^2 + \lambda |x|,\]</span> where <span class="math inline">\(a\in\mathbb{R}\)</span> and <span class="math inline">\(\lambda &gt;0\)</span>.</p>
<p>How to find <span class="math inline">\(x^*\)</span> that minimizes <span class="math inline">\(f(x)\)</span>?</p>
<p>The solution} <span class="math inline">\(x^*\)</span> must satisfy
<span class="math display">\[\begin{align*}
0 &amp;= \frac{\partial}{\partial x} \bigl(x^* - a \bigr)^2 + \lambda \;\frac{\partial}{\partial x} |x^*| \\
&amp; = 2\bigl(x^* - a \bigr) +\lambda z^*
\end{align*}\]</span>
where <span class="math inline">\(z^*\)</span> is the sub-gradient} of the absolute value function evaluated at <span class="math inline">\(x^*\)</span>, which equals to <span class="math inline">\(sign(x^*)\)</span> if <span class="math inline">\(x^*\neq 0\)</span> and any number in [-1,1], if <span class="math inline">\(x^*=0\)</span>.</p>
<p>How to solve an one-dim LASSO?</p>
<p>The minimizer of <span class="math inline">\(f(x) = \bigl( x-a \bigr )^2 + \lambda |x|\)</span> is given by
<span class="math display">\[x^* = S_{\lambda/2}(}a)} = sign(a) \bigl(|a|-\lambda/2 \bigr)_{+} =
\begin{cases}
&amp; a-\lambda/2,  \text{ if } \,\,  a \; &gt;\lambda/2\\
&amp; 0, \qquad \quad \text{ if } |a| \leq \lambda/2\\
&amp; a+\lambda/2,  \text{ if } \,\,  a \; &lt; -\lambda/2\\
\end{cases}
\]</span>
<span class="math inline">\(S_{\lambda/2}(\cdot)\)</span>} is often referred to as the soft-thresholding operator.</p>
<p>How to solve an one-dim LASSO?</p>
<p>When the design matrix <span class="math inline">\(\mathbf{X}\)</span> is orthogonal, the LASSO solution is given by</p>
<p><span class="math display">\[\hat{\beta}_{j}^{lasso} = \begin{cases}
&amp; sign (\hat{\beta}_{j}^{ls} - \lambda/2), \text{ if }|\hat{\beta}_{j}^{ls}| &gt; \lambda/2\\
&amp; 0, \text{ if }|\hat{\beta}_{j}^{ls}| \leq \lambda/2\\
\end{cases}
\]</span></p>
<p>A large <span class="math inline">\(\lambda\)</span> will cause some of the coefficients to be exactly zero. So, LASSO does both variable (subset) selection and (soft) shrinkage.</p>
<p>LASSO regression</p>
<p>Remarks</p>
<p>Use LASSO when the effect of predictors is sparse.
This means that only few predictors will have an effect on the response (e.g. gene expression data) or when number of predictors is large (<span class="math inline">\(p&gt;n\)</span>)</p>
<p>Select <span class="math inline">\(t\)</span> in the constraint <span class="math inline">\(\sum_{j=1}^p |\beta|_j\leq t\)</span> by using Cross-Validation (CV).</p>
<p>As <span class="math inline">\(t\)</span> increases, the number of predictors increases.</p>
<p>Comparing LASSO &amp; Ridge Regression</p>
<p>Comparing LASSO &amp; Ridge Regression</p>

<p>Comparing LASSO &amp; Ridge Regression</p>
<p>LASSO selects a subset of predictors <span class="math inline">\(\longrightarrow\)</span> some coefficients equal to zero.</p>
<p>Ridge regression performs better when the response is a function of many predictors with coefficients around the same size.</p>
<p>LASSO will perform better when a relatively small number of predictors have large coefficients and the rest are very small or equal to zero.</p>
<p>Since the number of predictors is never known a priori, cross-validation can be used to decide which approach is better for a particular data set.</p>
<p>LASSO with <span class="math inline">\(p&gt;n\)</span></p>
<p>When <span class="math inline">\(\mathbf{X}\)</span> is of full rank:</p>
<p>the LASSO solution is the minimizer of a convex} function over a convex} set<br />
the LASSO solution is unique since the first term is a strictly convex function.</p>
<p>When <span class="math inline">\(\mathbf{X}\)</span> is not of full rank, or when <span class="math inline">\(p&gt;n\)</span>:</p>
<p>the first term is no longer strictly convex.<br />
Then, <span class="math inline">\(\hat{\beta}_{lasso}\)</span> may be</p>
<p>unique if <span class="math inline">\(\mathbf{X}_S\)</span> is of full rank where <span class="math inline">\(S\)</span> is selected variable set, or
not unique, however <span class="math inline">\(\mathbf{X}\hat{\beta}_{lasso}\)</span> and <span class="math inline">\(|\hat{\beta}_{lasso}|\)</span> are still unique.</p>

</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="shrinkage-methods.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans serif",
    "size": 1
  },
  "edit": {
    "link": "https://github.com/USERNAME/REPO/edit/BRANCH/03-variableselection.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["_main.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section",
    "toc_depth": 4,
    "toc_float": true,
    "scroll_highlight": true
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
