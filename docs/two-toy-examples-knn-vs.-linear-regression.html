<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>1.5 Two Toy Examples: \(k\)NN vs. Linear Regression | CS 598 Practical Statistical Learning</title>
  <meta name="description" content="Lecture notes for “CS 598 PSL”." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="1.5 Two Toy Examples: \(k\)NN vs. Linear Regression | CS 598 Practical Statistical Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for “CS 598 PSL”." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="1.5 Two Toy Examples: \(k\)NN vs. Linear Regression | CS 598 Practical Statistical Learning" />
  
  <meta name="twitter:description" content="Lecture notes for “CS 598 PSL”." />
  

<meta name="author" content="Alexandra Chronopoulou" />


<meta name="date" content="2025-08-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bias-variance-trade-off.html"/>
<link rel="next" href="linear-regression-models.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">CS 598 PSL A. Chronopoulou</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><font color="darkblue"> Course Information </font></a></li>
<li class="chapter" data-level="1" data-path="introduction-to-statistical-learning.html"><a href="introduction-to-statistical-learning.html"><i class="fa fa-check"></i><b>1</b> Introduction to Statistical Learning</a>
<ul>
<li class="chapter" data-level="1.1" data-path="examples-of-statistical-learning-problems.html"><a href="examples-of-statistical-learning-problems.html"><i class="fa fa-check"></i><b>1.1</b> Examples of Statistical Learning Problems</a></li>
<li class="chapter" data-level="1.2" data-path="supervised-learning-framework.html"><a href="supervised-learning-framework.html"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning Framework</a></li>
<li class="chapter" data-level="1.3" data-path="why-is-statistical-learning-is-challenging.html"><a href="why-is-statistical-learning-is-challenging.html"><i class="fa fa-check"></i><b>1.3</b> Why is Statistical Learning is Challenging</a></li>
<li class="chapter" data-level="1.4" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>1.4</b> Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="1.5" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html"><i class="fa fa-check"></i><b>1.5</b> Two Toy Examples: <span class="math inline">\(k\)</span>NN vs. Linear Regression</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>1.5.1</b> <span class="math inline">\(k\)</span>-Nearest Neighbors</a></li>
<li class="chapter" data-level="1.5.2" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html#linear-regression"><i class="fa fa-check"></i><b>1.5.2</b> Linear Regression</a></li>
<li class="chapter" data-level="1.5.3" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html#simulated-binary-classification-example"><i class="fa fa-check"></i><b>1.5.3</b> Simulated Binary Classification Example</a></li>
<li class="chapter" data-level="1.5.4" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html#code-for-the-examples-in-the-lectures"><i class="fa fa-check"></i><b>1.5.4</b> Code for the Examples in the Lectures</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression-models.html"><a href="linear-regression-models.html"><i class="fa fa-check"></i><b>2</b> Linear Regression Models</a>
<ul>
<li class="chapter" data-level="2.1" data-path="multiple-linear-regression-mlr-model.html"><a href="multiple-linear-regression-mlr-model.html"><i class="fa fa-check"></i><b>2.1</b> Multiple Linear Regression (MLR) Model</a></li>
<li class="chapter" data-level="2.2" data-path="mlr-model-fitting.html"><a href="mlr-model-fitting.html"><i class="fa fa-check"></i><b>2.2</b> MLR Model Fitting</a></li>
<li class="chapter" data-level="2.3" data-path="least-squares-normal-equations.html"><a href="least-squares-normal-equations.html"><i class="fa fa-check"></i><b>2.3</b> Least-Squares &amp; Normal Equations</a></li>
<li class="chapter" data-level="2.4" data-path="goodness-of-fit-r-square.html"><a href="goodness-of-fit-r-square.html"><i class="fa fa-check"></i><b>2.4</b> Goodness-Of-Fit: <span class="math inline">\(R\)</span>-Square</a></li>
<li class="chapter" data-level="2.5" data-path="linear-transformations-on-x.html"><a href="linear-transformations-on-x.html"><i class="fa fa-check"></i><b>2.5</b> Linear Transformations on <span class="math inline">\(X\)</span></a></li>
<li class="chapter" data-level="2.6" data-path="rank-deficiency.html"><a href="rank-deficiency.html"><i class="fa fa-check"></i><b>2.6</b> Rank deficiency</a></li>
<li class="chapter" data-level="2.7" data-path="hypothesis-testing-in-mlr.html"><a href="hypothesis-testing-in-mlr.html"><i class="fa fa-check"></i><b>2.7</b> Hypothesis Testing in MLR</a></li>
<li class="chapter" data-level="2.8" data-path="categorical-variables-in-mlr.html"><a href="categorical-variables-in-mlr.html"><i class="fa fa-check"></i><b>2.8</b> Categorical Variables in MLR</a></li>
<li class="chapter" data-level="2.9" data-path="collinearity.html"><a href="collinearity.html"><i class="fa fa-check"></i><b>2.9</b> Collinearity</a></li>
<li class="chapter" data-level="2.10" data-path="model-diagnostics.html"><a href="model-diagnostics.html"><i class="fa fa-check"></i><b>2.10</b> Model Diagnostics</a></li>
<li class="chapter" data-level="2.11" data-path="the-birthweight-data-set-example.html"><a href="the-birthweight-data-set-example.html"><i class="fa fa-check"></i><b>2.11</b> The <code>Birthweight</code> Data Set Example</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="variable-selection-regularization.html"><a href="variable-selection-regularization.html"><i class="fa fa-check"></i><b>3</b> Variable Selection &amp; Regularization</a>
<ul>
<li class="chapter" data-level="3.1" data-path="training-vs.-testing-errors.html"><a href="training-vs.-testing-errors.html"><i class="fa fa-check"></i><b>3.1</b> Training vs. Testing Errors</a></li>
<li class="chapter" data-level="3.2" data-path="subset-selection.html"><a href="subset-selection.html"><i class="fa fa-check"></i><b>3.2</b> Subset Selection</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="subset-selection.html"><a href="subset-selection.html#information-criteria-based-procedures"><i class="fa fa-check"></i><b>3.2.1</b> Information Criteria-based procedures</a></li>
<li class="chapter" data-level="3.2.2" data-path="subset-selection.html"><a href="subset-selection.html#search-algorithms"><i class="fa fa-check"></i><b>3.2.2</b> Search Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html"><i class="fa fa-check"></i><b>3.3</b> Shrinkage Methods</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#ridge-regression"><i class="fa fa-check"></i><b>3.3.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="3.3.2" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#lasso-regression"><i class="fa fa-check"></i><b>3.3.2</b> Lasso Regression</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-student-performance-example.html"><a href="the-student-performance-example.html"><i class="fa fa-check"></i><b>3.4</b> The <code>Student Performance</code> Example</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="nonlinear-regression.html"><a href="nonlinear-regression.html"><i class="fa fa-check"></i><b>4</b> NonLinear Regression</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="nonlinear-regression.html"><a href="nonlinear-regression.html#a-note-on-nonlinearity"><i class="fa fa-check"></i><b>4.0.1</b> A Note on Nonlinearity</a></li>
<li class="chapter" data-level="4.1" data-path="polynomial-regression.html"><a href="polynomial-regression.html"><i class="fa fa-check"></i><b>4.1</b> Polynomial Regression</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="polynomial-regression.html"><a href="polynomial-regression.html#polynomial-basis-functions"><i class="fa fa-check"></i><b>4.1.1</b> Polynomial Basis Functions</a></li>
<li class="chapter" data-level="4.1.2" data-path="polynomial-regression.html"><a href="polynomial-regression.html#polynomial-regression-1"><i class="fa fa-check"></i><b>4.1.2</b> Polynomial Regression</a></li>
<li class="chapter" data-level="4.1.3" data-path="polynomial-regression.html"><a href="polynomial-regression.html#orthogonal-polynomials"><i class="fa fa-check"></i><b>4.1.3</b> Orthogonal Polynomials</a></li>
<li class="chapter" data-level="4.1.4" data-path="polynomial-regression.html"><a href="polynomial-regression.html#piece-wise-polynomials"><i class="fa fa-check"></i><b>4.1.4</b> Piece-wise Polynomials</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="splines-regression.html"><a href="splines-regression.html"><i class="fa fa-check"></i><b>4.2</b> Splines Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="splines-regression.html"><a href="splines-regression.html#examples-of-cubic-splines-basis"><i class="fa fa-check"></i><b>4.2.1</b> Examples of Cubic Splines Basis</a></li>
<li class="chapter" data-level="4.2.2" data-path="splines-regression.html"><a href="splines-regression.html#b-splines-basis-functions-in-r"><i class="fa fa-check"></i><b>4.2.2</b> B-Splines Basis Functions in <code>R</code></a></li>
<li class="chapter" data-level="4.2.3" data-path="splines-regression.html"><a href="splines-regression.html#natural-cubic-splines-ncs"><i class="fa fa-check"></i><b>4.2.3</b> Natural Cubic Splines (NCS)</a></li>
<li class="chapter" data-level="4.2.4" data-path="splines-regression.html"><a href="splines-regression.html#regression-splines"><i class="fa fa-check"></i><b>4.2.4</b> Regression Splines</a></li>
<li class="chapter" data-level="4.2.5" data-path="splines-regression.html"><a href="splines-regression.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>4.2.5</b> K-Fold Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="smoothing-splines.html"><a href="smoothing-splines.html"><i class="fa fa-check"></i><b>4.3</b> Smoothing Splines</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="smoothing-splines.html"><a href="smoothing-splines.html#the-roughness-penalty-approach"><i class="fa fa-check"></i><b>4.3.1</b> The Roughness Penalty Approach</a></li>
<li class="chapter" data-level="4.3.2" data-path="smoothing-splines.html"><a href="smoothing-splines.html#proof"><i class="fa fa-check"></i><b>4.3.2</b> Proof</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="fitting-smoothing-splines.html"><a href="fitting-smoothing-splines.html"><i class="fa fa-check"></i><b>4.4</b> Fitting Smoothing Splines</a></li>
<li class="chapter" data-level="4.5" data-path="the-birthrates-example-in-r.html"><a href="the-birthrates-example-in-r.html"><i class="fa fa-check"></i><b>4.5</b> The <code>Birthrates</code> Example in R</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">CS 598 Practical Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="two-toy-examples-knn-vs.-linear-regression" class="section level2 hasAnchor" number="1.5">
<h2><span class="header-section-number">1.5</span> Two Toy Examples: <span class="math inline">\(k\)</span>NN vs. Linear Regression<a href="two-toy-examples-knn-vs.-linear-regression.html#two-toy-examples-knn-vs.-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before we wrap up the introduction, we will review two simple supervised learning examples <br>
(i) <span class="math inline">\(k\)</span>-Nearest Neighbors (<span class="math inline">\(k\)</span>NN) <br>
(ii) Linear Regression<br />
and will examine their performance and understand the bias-variance trade-off.</p>
<p><br></p>
<div id="k-nearest-neighbors" class="section level3 hasAnchor" number="1.5.1">
<h3><span class="header-section-number">1.5.1</span> <span class="math inline">\(k\)</span>-Nearest Neighbors<a href="two-toy-examples-knn-vs.-linear-regression.html#k-nearest-neighbors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the <strong><span class="math inline">\(k\)</span>-Nearest Neighbors</strong> (<span class="math inline">\(k\)</span>NN) method, we use observations in the training set that are closest to <span class="math inline">\(\mathbf{x}\)</span> to form <span class="math inline">\(\mathbf{y}\)</span>. Specifically, the <span class="math inline">\(k\)</span>-Nearest Neighbor fit for <span class="math inline">\(\hat{y}\)</span> is
<font color="darkblue">
<span class="math display">\[\hat{\mathbf{y}}(\mathbf{x}) = \frac{1}{k} \sum_{x_i \in N_k(\mathbf{x})} y_i\]</span>
</font>
where <span class="math inline">\(N_k(\mathbf{x})\)</span> is the neighborhood of <span class="math inline">\(\mathbf{x}\)</span> defined by the <span class="math inline">\(k\)</span> closest points <span class="math inline">\(x_i\)</span> in the training sample. In a regression context the <span class="math inline">\(k\)</span>NN fitted <span class="math inline">\(\mathbf{\hat{y}}\)</span> predicts <span class="math inline">\(\mathbf{y}\)</span> via a <strong>local average</strong>, while in the classification context <span class="math inline">\(k\)</span>NN returns the <strong>majority vote</strong> in <span class="math inline">\(N_k(\mathbf{x})\)</span> or a probability calculated on the frequencies in <span class="math inline">\(N_k(\mathbf{x})\)</span>. What can be challenging in the <span class="math inline">\(k\)</span>NN approach is <em>tuning</em> <span class="math inline">\(k\)</span>, the neighborhood size, and determining the metric to define the neighborhood.</p>
<ul>
<li><p>The choice of <span class="math inline">\(k\)</span> is directly linked to the complexity of the method which is roughly equal to <span class="math inline">\(n/k\)</span>.</p>
<ul>
<li><p>When <span class="math inline">\(k=1\)</span>, the prediction at <span class="math inline">\(x_i\)</span> is <strong>exactly</strong> <span class="math inline">\(y_i\)</span> which means that we have zero training error.</p></li>
<li><p>When <span class="math inline">\(k=n\)</span>, every neighborhood contains all the <span class="math inline">\(n\)</span> training samples, so the prediction is the same no matter <span class="math inline">\(x\)</span>.</p></li>
</ul></li>
<li><p>The default metric to define the neighborhood is the <em>Euclidean distance</em>:
<font color="darkblue">
<span class="math display">\[d\bigl( \mathbf{x}, \tilde{\mathbf{x}} \bigr) = \sum_{j=1}^{p} w_j \bigl( x_j - \tilde{x}_j \bigr)^2,\]</span>
</font>
where we would like to learn the <span class="math inline">\(w_j\)</span>’s from the data.</p></li>
</ul>
<p><br></p>
</div>
<div id="linear-regression" class="section level3 hasAnchor" number="1.5.2">
<h3><span class="header-section-number">1.5.2</span> Linear Regression<a href="two-toy-examples-knn-vs.-linear-regression.html#linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Given a vector of inputs <span class="math inline">\(\mathbf{x}^T = (x_1, x_2, \ldots, x_p)\)</span>, we <strong>approximate</strong> <span class="math inline">\(Y\)</span> via a <em>linear</em> function
<font color="darkblue">
<span class="math display">\[f(\mathbf{x}) \approx \beta_0 + \sum_{j=1}^{p} x_j \beta_j\]</span>
</font>
Our goal is to <em>estimate</em> the parameters <span class="math inline">\(\beta_j\)</span> using the Least-Squares (LS) method by minimizing the Residual Sum of Squares (<em>objective function</em>)
<font color="darkblue">
<span class="math display">\[\min_{\beta_0, \ldots, \beta_p} \sum_{i=1}^{n} \Bigl( y_i - \beta_0 - x_{i1}\beta_1 - \ldots - x_{ip} \beta_p \Bigr)^2\]</span>
</font>
The solution is easy to obtain (both in R/Python and analytically under certain assumptions) and the fitted value for the <span class="math inline">\(i\)</span>th input <span class="math inline">\(x_i\)</span> is given by
<font color="darkblue">
<span class="math display">\[\hat{y}_i = \hat{y}(x_i) = x_i^T \hat{\beta}\]</span>
</font>
We leave the details to be discussed in Week 2.</p>
<p><br></p>
<div id="linear-regression-in-a-classification-context" class="section level4 unnumbered hasAnchor">
<h4>Linear Regression in a Classification Context<a href="two-toy-examples-knn-vs.-linear-regression.html#linear-regression-in-a-classification-context" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>We can apply linear regression on classification problems with <span class="math inline">\(Y=0 \text{ or } 1\)</span>. In this case, we predict <span class="math inline">\(Y\)</span> to be 1 if the LS prediction <span class="math inline">\(f(x)\)</span> is bigger than 0.5, and 0 otherwise. This approach has drawbacks. First of all, the squared difference <span class="math inline">\(p(\mathbf{x}) = \Bigl(y_i - f(\mathbf{x}_i) \Bigr)^2\)</span> is not a good evaluation metric, since considering a linear function <span class="math inline">\(f(\mathbf{x})\)</span> may result in values outside <span class="math inline">\([0,1]\)</span>. Therefore, when we are in this context a Logistic regression is the gold standard according to which
<span class="math display">\[\log \frac{p(\mathbf{x})}{1-p(\mathbf{x}) } \approx \beta_0 + \sum_{j=1}^{p} x_j \beta_j\]</span></p>
<p>More on this in Week 8.
<br></p>
</div>
</div>
<div id="simulated-binary-classification-example" class="section level3 hasAnchor" number="1.5.3">
<h3><span class="header-section-number">1.5.3</span> Simulated Binary Classification Example<a href="two-toy-examples-knn-vs.-linear-regression.html#simulated-binary-classification-example" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider a response variable <span class="math inline">\(G\)</span> that takes two values (0 – BLUE or 1 – ORANGE). In our simulation, we generate 200 such values; 100 in each class, and our goal is to use both regression and <span class="math inline">\(k\)</span>NN to classify the data. The code behind the simulation and the plots can be found <a href="code/W1_kNNvsLR.html" target="_blank">here</a>.</p>
<p>We start by fitting a linear regression model to the simulated data. In a naive approach, we treat the response is a continuous variable. Hence, the continuous fitted values <span class="math inline">\(\hat{Y}\)</span> are converted to a fitted class variable <span class="math inline">\(\hat{G}\)</span> according to the following rule:
<span class="math display">\[\hat{G} = \begin{cases}
&amp; Blue, \text{ if } \hat{Y} &gt; 0.5\\
&amp; Orange, \text{ otherwise } \\
\end{cases} \]</span></p>
<p>Our classification example is in two dimensions which means that the decision boundary (the boundary that separates the orange from the blue region) is a straight line.</p>
<p><img src="images/week1/regclass.png" width="70%" style="display: block; margin: auto;" /></p>
<p>Specifically, the fitted decision boundary is a straight line (the black line in the plot) defined by <span class="math inline">\(\mathbf{x}^T \hat{\beta} = 0.5\)</span>. Based on our simulation, we know that the blue region should be above the black fitted regression line, while the orange region should be below the black fitted regression line. We observe that there are many misclassifications on both sides of the decision boundary.</p>
<p>The regression line seems to be very smooth and too rigid when it comes to classifying the data. On the other end of the spectrum, we have the <span class="math inline">\(k\)</span> Nearest-Neighbor approach. So, for the same simulated data, the nearest neighbor method will use the observations in the training set closest in input space to <span class="math inline">\(X\)</span> to form <span class="math inline">\(\hat{Y}\)</span>.</p>
<p>Using a 15-nearest-neighbor averaging of the binary coded response such that <span class="math inline">\(\hat{Y}\)</span> is the proportion of blue’s in the neighborhood, then we assign
<span class="math display">\[\hat{G} = \begin{cases}
&amp; Blue, \text{ if } \hat{Y} &gt; 0.5\\
&amp; Orange,\text{ otherwise } \\
\end{cases} \]</span></p>
<p>In this case, the predicted class is chosen by majority vote amont the 15 nearest neighbors.</p>
<p><img src="images/week1/kNN15.png" width="70%" style="display: block; margin: auto;" /></p>
<p>We observe that the decision boundary separating the blue from the orange region is far more irregular than before and sensitive to local clusters of blue and orange dots. As a result, we have fewer misclassified observations than before. Remember that we can tune the neighborhood size. So, if we take the extreme scenario in which <span class="math inline">\(k=1\)</span> and we only consider <strong>one</strong> neighbor, we have</p>
<p><img src="images/week1/kNN1.png" width="70%" style="display: block; margin: auto;" /></p>
<p>This results in a much rougher decision boundary with hardly any misclassified data. However, is this the ideal choice? The answer is no, but in order to understand the reason we need to consider the <em>generalization error</em>.</p>
<p>Up to now, we have used the same data for training and comparison purposes. As a result, a method like <span class="math inline">\(k\)</span>NN has seemingly 0 error for <span class="math inline">\(k=1\)</span>. In order to make fair comparisons, we should consider another data set, independent of the one used to fit the data, so that we can compute the test/generalization error for both methods.</p>
<p>In the plot below, we compare the misclassification error on the testing data set as a function of the degrees of freedom. In other words, we compare several <span class="math inline">\(k\)</span>NN fits (for different <span class="math inline">\(k\)</span>s) and the regression fit. The testing set here contains 10,000 observations.</p>
<p><img src="images/week1/traintesterror.png" width="70%" style="display: block; margin: auto;" /></p>
<p>The magenta curve is the test error and the blue curve is the training error for the <span class="math inline">\(k\)</span>NN classification for different <span class="math inline">\(k\)</span>s. In our simulated example, we used a 5-fold cross validation method to identify the optimal <span class="math inline">\(k\)</span>. The results for the optimal <span class="math inline">\(k\)</span> in <span class="math inline">\(k\)</span>NN are denoted with a diamond. The results for linear regression are the
magenta and blue triangles at 3 DFs (the DFs were determined based on the dimension of the linear model we fitted).</p>
<p>The regression method has the advantage of it being linear with only <span class="math inline">\(p=3\)</span> parameters to estimate which means that it has a relatively <em>low variance</em>. However, the linearity assumption seems to be quite restrictive for the classification problem under consideration, which means that we expect to have high bias.</p>
<p>On the other hand, the <span class="math inline">\(k\)</span>NN approach has no assumption of the shape of the underlying <span class="math inline">\(f\)</span>, maybe except some local smoothness. This flexibility results in overfitting and a low bias (we saw that in the extreme case of <span class="math inline">\(k=1\)</span>).It can be shown that as <span class="math inline">\(k, n \rightarrow \infty\)</span> such that <span class="math inline">\(k/n \rightarrow 0\)</span>, <span class="math inline">\(k\)</span>NN is consistent. At the same time, the method has higher variance with the extreme case of when the number of parameters for <span class="math inline">\(k\)</span>NN is roughly <span class="math inline">\(n=k\)</span>, which goes to <span class="math inline">\(\infty\)</span> in order to achieve consistency.</p>
</div>
<div id="code-for-the-examples-in-the-lectures" class="section level3 hasAnchor" number="1.5.4">
<h3><span class="header-section-number">1.5.4</span> Code for the Examples in the Lectures<a href="two-toy-examples-knn-vs.-linear-regression.html#code-for-the-examples-in-the-lectures" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We replicate one of the examples in the ESL book to illustrate the differences between the two simplest prediction methods for binary outcomes: the <em>Regression method</em> and the <em><span class="math inline">\(k\)</span>-Nearest-Neighbors method</em>.</p>
<p></br></p>
<div id="simulation" class="section level4 hasAnchor" number="1.5.4.1">
<h4><span class="header-section-number">1.5.4.1</span> Simulation<a href="two-toy-examples-knn-vs.-linear-regression.html#simulation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<ul>
<li>The <em>generated data</em> consist of a <strong>binary classification</strong> with two classes, labeled as <code>0</code> and <code>1</code>.</li>
<li>The <em>features</em> are two-dimensional.</li>
<li><code>Class 1</code> data points are generated from a Gaussian distribution with mean <span class="math inline">\(\mu_1\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, i.e. <span class="math inline">\(X_{1,1}, \ldots, X_{1,n_1} \sim \mathbb{N}\bigl( \mu_1, \sigma^2 \bigr)\)</span>.</li>
<li><code>Class 0</code> data points are generated from a Gaussian distribution with mean <span class="math inline">\(\mu_2\)</span> and variance <span class="math inline">\(\sigma^2\)</span>,i.e. <span class="math inline">\(X_{01}, \ldots, X_{0n_2} \sim \mathbb{N}\bigl( \mu_0, \sigma^2 \bigr)\)</span>.</li>
<li>In total, we generate <em>200 training samples</em> (<span class="math inline">\(n=100\)</span> for each class), and we assign labels to the training data (100 <code>Class 1</code> and 100 <code>Class 0</code>).</li>
<li>Similarly, we generate <em>10,000 test samples</em>.</li>
</ul>
<p></br></p>
</div>
<div id="code-for-the-simulation" class="section level4 hasAnchor" number="1.5.4.2">
<h4><span class="header-section-number">1.5.4.2</span> Code for the Simulation<a href="two-toy-examples-knn-vs.-linear-regression.html#code-for-the-simulation" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>Set the model parameters for the simulation:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb1-1" tabindex="-1"></a>p <span class="ot">=</span> <span class="dv">2</span>;           <span class="do">## No. of parameters</span></span>
<span id="cb1-2"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb1-2" tabindex="-1"></a>sigma <span class="ot">=</span> <span class="dv">1</span>;       <span class="do">## St. Dev for the Normals (common)</span></span>
<span id="cb1-3"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb1-3" tabindex="-1"></a>mu1 <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">0</span>);   <span class="do">## Vector of means for the first Normal</span></span>
<span id="cb1-4"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb1-4" tabindex="-1"></a>mu0 <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>);   <span class="do">## Vector of means for the second Normal</span></span></code></pre></div>
<p><br>
Generate <span class="math inline">\(n\)</span> i.i.d. (independent and identically distributed) samples from each normal to create the <em>training</em> data set.</p>
<p><br></p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-1" tabindex="-1"></a>n <span class="ot">=</span> <span class="dv">100</span>;         <span class="do">## Training Sample Size for each Normal</span></span>
<span id="cb2-2"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-2" tabindex="-1"></a></span>
<span id="cb2-3"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-3" tabindex="-1"></a><span class="do">## rnorm(2*n*p) generates 2*n*p  N(0,1) random variables.</span></span>
<span id="cb2-4"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-4" tabindex="-1"></a><span class="do">## matrix(rnorm(2*n*p), 2*n, p)*sigma generates a 2n-by-p matrix of two N(0, sigma^2) each of length 2*n.</span></span>
<span id="cb2-5"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-5" tabindex="-1"></a><span class="do">## Adding matrix(rep(mu1, n), nrow=n,byrow=TRUE) to each column of the previous matrix</span></span>
<span id="cb2-6"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-6" tabindex="-1"></a><span class="do">##    shifts each of the columns to generate the Normals with means mu1 or mu0.</span></span>
<span id="cb2-7"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-7" tabindex="-1"></a><span class="do">## Note that both mu0, mu1 are 2-dimensional.</span></span>
<span id="cb2-8"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-8" tabindex="-1"></a></span>
<span id="cb2-9"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-9" tabindex="-1"></a>traindata <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">2</span><span class="sc">*</span>n<span class="sc">*</span>p), <span class="dv">2</span><span class="sc">*</span>n, p)<span class="sc">*</span>sigma <span class="sc">+</span> </span>
<span id="cb2-10"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-10" tabindex="-1"></a>            <span class="fu">rbind</span>(<span class="fu">matrix</span>(<span class="fu">rep</span>(mu1, n), <span class="at">nrow=</span>n,<span class="at">byrow=</span><span class="cn">TRUE</span>), </span>
<span id="cb2-11"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-11" tabindex="-1"></a>            <span class="fu">matrix</span>(<span class="fu">rep</span>(mu0, n), <span class="at">nrow=</span>n, <span class="at">byrow=</span><span class="cn">TRUE</span>))</span>
<span id="cb2-12"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-12" tabindex="-1"></a></span>
<span id="cb2-13"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-13" tabindex="-1"></a><span class="co"># dim(traindata)</span></span>
<span id="cb2-14"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-14" tabindex="-1"></a></span>
<span id="cb2-15"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-15" tabindex="-1"></a><span class="do">## We generate the 0 or 1 labels.</span></span>
<span id="cb2-16"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb2-16" tabindex="-1"></a>Ytrain <span class="ot">=</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>,n), <span class="fu">rep</span>(<span class="dv">0</span>,n)))</span></code></pre></div>
<p><br>
Generate <span class="math inline">\(N\)</span> <em>test</em> samples in a similar way:</p>
<p><br></p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb3-1" tabindex="-1"></a>N<span class="ot">=</span><span class="dv">10000</span>;  </span>
<span id="cb3-2"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb3-3" tabindex="-1"></a>testdata <span class="ot">=</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(<span class="dv">2</span><span class="sc">*</span>N<span class="sc">*</span>p), <span class="dv">2</span><span class="sc">*</span>N, p)<span class="sc">*</span>sigma<span class="sc">+</span> </span>
<span id="cb3-4"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb3-4" tabindex="-1"></a>           <span class="fu">rbind</span>(<span class="fu">matrix</span>(<span class="fu">rep</span>(mu1, N), <span class="at">nrow=</span>N,<span class="at">byrow=</span><span class="cn">TRUE</span>), </span>
<span id="cb3-5"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb3-5" tabindex="-1"></a>           <span class="fu">matrix</span>(<span class="fu">rep</span>(mu0, N), <span class="at">nrow=</span>N, <span class="at">byrow=</span><span class="cn">TRUE</span>))</span>
<span id="cb3-6"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb3-7" tabindex="-1"></a>Ytest <span class="ot">=</span> <span class="fu">factor</span>(<span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">1</span>,N), <span class="fu">rep</span>(<span class="dv">0</span>,N)))</span></code></pre></div>
<br>
<hr>
<p><br></p>
</div>
<div id="visualization-of-the-simulated-data" class="section level4 hasAnchor" number="1.5.4.3">
<h4><span class="header-section-number">1.5.4.3</span> Visualization of the Simulated Data<a href="two-toy-examples-knn-vs.-linear-regression.html#visualization-of-the-simulated-data" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><em>This section also serves as a review of plotting in R.</em></p>
<p><br></p>
<p>We visualize the data we generated – those in the <code>traindata</code> matrix.
<br></p>
<p>In the figure generated by the code below, points from two groups are colored in orange and blue, respectively; the two centers are plotted as +, and a legend is added to explain the association of each color.</p>
<p><br></p>
</div>
<div id="using-the-default-r-functions-for-plotting" class="section level4 hasAnchor" number="1.5.4.4">
<h4><span class="header-section-number">1.5.4.4</span> Using the default R functions for plotting<a href="two-toy-examples-knn-vs.-linear-regression.html#using-the-default-r-functions-for-plotting" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-1" tabindex="-1"></a><span class="do">## Create an empty plotting area: The axes are the two vectors of normals generated, </span></span>
<span id="cb4-2"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-2" tabindex="-1"></a><span class="do">## each one saved in a column of the `traindata` matrix.</span></span>
<span id="cb4-3"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-3" tabindex="-1"></a><span class="do">## The following line creates an empty plot, since we used the option type=&quot;n&quot;</span></span>
<span id="cb4-4"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-4" tabindex="-1"></a><span class="do">## We do this so that we can color-code the data.</span></span>
<span id="cb4-5"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-5" tabindex="-1"></a><span class="fu">plot</span>(traindata[,<span class="dv">1</span>], traindata[,<span class="dv">2</span>], <span class="at">type=</span><span class="st">&quot;n&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;&quot;</span>);</span>
<span id="cb4-6"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-7" tabindex="-1"></a><span class="co"># Add the &quot;Class 1&quot; points - in blue color.</span></span>
<span id="cb4-8"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-8" tabindex="-1"></a><span class="fu">points</span>(traindata[<span class="dv">1</span><span class="sc">:</span>n, <span class="dv">1</span>], traindata[<span class="dv">1</span><span class="sc">:</span>n,<span class="dv">2</span>], <span class="at">pch=</span><span class="dv">16</span>, <span class="at">col=</span><span class="st">&quot;blue&quot;</span>);</span>
<span id="cb4-9"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-10" tabindex="-1"></a><span class="co"># Add the &quot;Class 0&quot; points - in orange color</span></span>
<span id="cb4-11"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-11" tabindex="-1"></a><span class="fu">points</span>(traindata[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(<span class="dv">2</span><span class="sc">*</span>n),<span class="dv">1</span>], traindata[(n<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>(<span class="dv">2</span><span class="sc">*</span>n),<span class="dv">2</span>], <span class="at">pch=</span><span class="dv">16</span>, <span class="at">col=</span><span class="st">&quot;orange&quot;</span>); </span>
<span id="cb4-12"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-12" tabindex="-1"></a></span>
<span id="cb4-13"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-13" tabindex="-1"></a><span class="co"># Add the  centers for class 1</span></span>
<span id="cb4-14"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-14" tabindex="-1"></a><span class="fu">points</span>(mu1[<span class="dv">1</span>], mu1[<span class="dv">2</span>], <span class="at">pch=</span><span class="dv">13</span>, <span class="at">cex=</span><span class="fl">1.5</span>, <span class="at">col=</span><span class="st">&quot;green&quot;</span>);    </span>
<span id="cb4-15"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-15" tabindex="-1"></a></span>
<span id="cb4-16"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-16" tabindex="-1"></a><span class="co"># Add the  centers for class 0</span></span>
<span id="cb4-17"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-17" tabindex="-1"></a><span class="fu">points</span>(mu0[<span class="dv">1</span>], mu0[<span class="dv">2</span>], <span class="at">pch=</span><span class="dv">13</span>, <span class="at">cex=</span><span class="fl">1.5</span>, <span class="at">col=</span><span class="st">&quot;red&quot;</span>);   </span>
<span id="cb4-18"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-18" tabindex="-1"></a></span>
<span id="cb4-19"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-19" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;bottomright&quot;</span>, <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">16</span>,<span class="dv">16</span>), <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;orange&quot;</span>), </span>
<span id="cb4-20"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb4-20" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Class 1&quot;</span>, <span class="st">&quot;Class 0&quot;</span>))</span></code></pre></div>
<p><img src="my-598PSL_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="using-the-ggplot2-r-library" class="section level4 hasAnchor" number="1.5.4.5">
<h4><span class="header-section-number">1.5.4.5</span> Using the <code>ggplot2</code> R library<a href="two-toy-examples-knn-vs.-linear-regression.html#using-the-ggplot2-r-library" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In this section, we present an alternative way to construct the plot above via the <code>ggplot2</code> package. The <code>ggplot2</code> package allows us to create elaborate plots. More information can be found here: <a href="http://ggplot2.org/" class="uri">http://ggplot2.org/</a></p>
<p><br></p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-1" tabindex="-1"></a><span class="co"># install.package(&quot;ggplot2&quot;)</span></span>
<span id="cb5-2"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-2" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;ggplot2&quot;</span>)</span>
<span id="cb5-3"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-4" tabindex="-1"></a><span class="do">## The input in a `ggplot` function can only be a data.frame</span></span>
<span id="cb5-5"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-5" tabindex="-1"></a></span>
<span id="cb5-6"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-6" tabindex="-1"></a><span class="do">## In our case the data is a matrix, so we convert them to data.frames here:</span></span>
<span id="cb5-7"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-7" tabindex="-1"></a>mytraindata <span class="ot">=</span> <span class="fu">data.frame</span>(<span class="at">X1=</span>traindata[,<span class="dv">1</span>], <span class="at">X2=</span>traindata[,<span class="dv">2</span>], <span class="at">Y=</span>Ytrain)</span>
<span id="cb5-8"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-8" tabindex="-1"></a></span>
<span id="cb5-9"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-9" tabindex="-1"></a><span class="do">## The ggplot output is an object which is saved in training.scatter.</span></span>
<span id="cb5-10"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-10" tabindex="-1"></a><span class="do">## In this object, we can later add --and plot-- additional features.</span></span>
<span id="cb5-11"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-11" tabindex="-1"></a></span>
<span id="cb5-12"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-12" tabindex="-1"></a>training.scatter <span class="ot">=</span> <span class="fu">ggplot</span>(mytraindata, <span class="fu">aes</span>(X1, X2)) <span class="sc">+</span>  <span class="do">## creates the empty plot</span></span>
<span id="cb5-13"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-13" tabindex="-1"></a>        <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">colour=</span>Y), <span class="at">size=</span><span class="dv">2</span>) <span class="sc">+</span> <span class="do">## adds the points color-coded by the labels in Y</span></span>
<span id="cb5-14"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-14" tabindex="-1"></a>        <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;orange&quot;</span>, <span class="st">&quot;blue&quot;</span>)) <span class="sc">+</span>  <span class="do">## change the default colors</span></span>
<span id="cb5-15"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-15" tabindex="-1"></a>        <span class="do">## Use geom_point to add the centers (as before)</span></span>
<span id="cb5-16"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-16" tabindex="-1"></a>        <span class="fu">geom_point</span>(<span class="at">data=</span><span class="fu">data.frame</span>(<span class="at">X1=</span>mu1[<span class="dv">1</span>], <span class="at">X2=</span>mu1[<span class="dv">2</span>]), <span class="fu">aes</span>(X1, X2), <span class="at">colour=</span><span class="st">&quot;green&quot;</span>, <span class="at">shape=</span><span class="dv">2</span>,<span class="at">size=</span><span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb5-17"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-17" tabindex="-1"></a>        <span class="fu">geom_point</span>(<span class="at">data=</span><span class="fu">data.frame</span>(<span class="at">X1=</span>mu0[<span class="dv">1</span>], <span class="at">X2=</span>mu0[<span class="dv">2</span>]), <span class="fu">aes</span>(X1, X2), <span class="at">colour=</span><span class="st">&quot;red&quot;</span>, <span class="at">shape=</span><span class="dv">2</span>, <span class="at">size=</span><span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb5-18"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-18" tabindex="-1"></a>        <span class="fu">ggtitle</span>(<span class="st">&quot;Simulated Training Data&quot;</span>)  <span class="sc">+</span> <span class="do">## add a title </span></span>
<span id="cb5-19"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-19" tabindex="-1"></a>        <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;&quot;</span>, <span class="at">y=</span><span class="st">&quot;&quot;</span>)  <span class="do">## remove axes labels</span></span>
<span id="cb5-20"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-20" tabindex="-1"></a>  </span>
<span id="cb5-21"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb5-21" tabindex="-1"></a><span class="fu">plot</span>(training.scatter)</span></code></pre></div>
<p><img src="my-598PSL_files/figure-html/unnamed-chunk-16-1.png" width="672" /></p>
</div>
<div id="k-nearest-neighbors-method" class="section level4 hasAnchor" number="1.5.4.6">
<h4><span class="header-section-number">1.5.4.6</span> <span class="math inline">\(k\)</span> Nearest-Neighbors Method<a href="two-toy-examples-knn-vs.-linear-regression.html#k-nearest-neighbors-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><br></p>
<p>To apply the <span class="math inline">\(k\)</span>-NN method, we need to choose <span class="math inline">\(k\)</span>. In the example below, we use the neighborhood sizes suggested from the textbook (ESL). We also apply and plot the results for <span class="math inline">\(k=1\)</span> and <span class="math inline">\(k=15\)</span>.</p>
<p><br></p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-1" tabindex="-1"></a><span class="fu">library</span>(<span class="st">&quot;class&quot;</span>) </span>
<span id="cb6-2"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-2" tabindex="-1"></a></span>
<span id="cb6-3"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-3" tabindex="-1"></a>neighbor_size <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">151</span>, <span class="dv">101</span>, <span class="dv">69</span>,  <span class="dv">45</span>, <span class="dv">31</span>, <span class="dv">21</span>, <span class="dv">11</span>, <span class="dv">7</span>, <span class="dv">5</span>, <span class="dv">3</span>, <span class="dv">1</span>); <span class="do">## These are different k&#39;s to try.</span></span>
<span id="cb6-4"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-4" tabindex="-1"></a>m <span class="ot">=</span> <span class="fu">length</span>(neighbor_size);  <span class="do">## needed to run our for-loop below.</span></span>
<span id="cb6-5"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-5" tabindex="-1"></a></span>
<span id="cb6-6"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-6" tabindex="-1"></a>train.err.knn <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,m);   <span class="do">## vector to store training error</span></span>
<span id="cb6-7"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-7" tabindex="-1"></a>test.err.knn <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>, m);   <span class="do">## vector to store testing error</span></span>
<span id="cb6-8"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-8" tabindex="-1"></a></span>
<span id="cb6-9"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-9" tabindex="-1"></a><span class="do">## knn is the R function that runs the kNN method. Output is a factor.</span></span>
<span id="cb6-10"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-10" tabindex="-1"></a></span>
<span id="cb6-11"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-11" tabindex="-1"></a><span class="cf">for</span>( j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>m){</span>
<span id="cb6-12"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-12" tabindex="-1"></a>  Ytrain.pred <span class="ot">=</span> <span class="fu">knn</span>(traindata, traindata, Ytrain, <span class="at">k=</span>neighbor_size[j])   <span class="do">## predictions for training data</span></span>
<span id="cb6-13"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-13" tabindex="-1"></a>  train.err.knn[j]<span class="ot">=</span> <span class="fu">sum</span>(Ytrain <span class="sc">!=</span> Ytrain.pred)<span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>n)     <span class="do">## mis-classification training error</span></span>
<span id="cb6-14"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-14" tabindex="-1"></a>  Ytest.pred <span class="ot">=</span> <span class="fu">knn</span>(traindata, testdata, Ytrain,<span class="at">k=</span>neighbor_size[j])      <span class="do">## predictions for testing data</span></span>
<span id="cb6-15"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-15" tabindex="-1"></a>  test.err.knn[j] <span class="ot">=</span> <span class="fu">sum</span>(Ytest <span class="sc">!=</span> Ytest.pred)<span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>N)       <span class="do">## mis-classification testing error</span></span>
<span id="cb6-16"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-16" tabindex="-1"></a>}</span>
<span id="cb6-17"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-17" tabindex="-1"></a></span>
<span id="cb6-18"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb6-18" tabindex="-1"></a><span class="fu">cbind</span>(train.err.knn, test.err.knn)    <span class="do">## matrix containing the train and test errors</span></span></code></pre></div>
<pre><code>##       train.err.knn test.err.knn
##  [1,]         0.155      0.23960
##  [2,]         0.170      0.23690
##  [3,]         0.150      0.23865
##  [4,]         0.145      0.23865
##  [5,]         0.140      0.24020
##  [6,]         0.140      0.23995
##  [7,]         0.160      0.24605
##  [8,]         0.160      0.24810
##  [9,]         0.160      0.25530
## [10,]         0.115      0.26455
## [11,]         0.000      0.29590</code></pre>
<p><br></p>
</div>
<div id="fold-cross-validation-for-choosing-optimal-k" class="section level4 hasAnchor" number="1.5.4.7">
<h4><span class="header-section-number">1.5.4.7</span> 5-Fold Cross-Validation for Choosing optimal <span class="math inline">\(k\)</span><a href="two-toy-examples-knn-vs.-linear-regression.html#fold-cross-validation-for-choosing-optimal-k" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>A systematic way to determine the <em>optimal</em> value for the neighborhood size, <span class="math inline">\(k\)</span>, in a range of values is the so-called 5-fold Cross-Validation (CV) method. Essentially, the method selects the <span class="math inline">\(k\)</span> value that minimizes the CV error. In a nutshell, the 5-fold CV error for each <span class="math inline">\(k\)</span> is a <em>sum of 5 prediction errors</em>, one corresponding to each fold.
<br></p>
<p>In the code below, we have an outside loop from 1 to 5 (the folds), and an inside loop from 1 to <span class="math inline">\(m\)</span> (all possible values for <span class="math inline">\(k\)</span>). Inside the loop, we use 80% (i.e., four folds) of the data as training and predict on the 20% (i.e., one fold) holdout set.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb8-1"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-1" tabindex="-1"></a><span class="do">## In this chunk of code, we use the same vector of k&#39;s as above - the neighbor_size vector of size m</span></span>
<span id="cb8-2"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-2" tabindex="-1"></a></span>
<span id="cb8-3"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-3" tabindex="-1"></a><span class="do">## Initialize a vector cv.error to store the CV error for each k.</span></span>
<span id="cb8-4"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-4" tabindex="-1"></a>cv.error <span class="ot">=</span> <span class="fu">rep</span>(<span class="dv">0</span>,m);</span>
<span id="cb8-5"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-5" tabindex="-1"></a></span>
<span id="cb8-6"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-6" tabindex="-1"></a></span>
<span id="cb8-7"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-7" tabindex="-1"></a>id <span class="ot">=</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span>(<span class="dv">2</span><span class="sc">*</span>n),(<span class="dv">2</span><span class="sc">*</span>n), <span class="at">replace=</span><span class="cn">FALSE</span>);</span>
<span id="cb8-8"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-8" tabindex="-1"></a>fold <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,  <span class="dv">40</span>,  <span class="dv">80</span>, <span class="dv">120</span>, <span class="dv">160</span>, <span class="dv">200</span>)</span>
<span id="cb8-9"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-9" tabindex="-1"></a></span>
<span id="cb8-10"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-10" tabindex="-1"></a><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>)</span>
<span id="cb8-11"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-11" tabindex="-1"></a>    <span class="cf">for</span>(j <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>m){</span>
<span id="cb8-12"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-12" tabindex="-1"></a>      </span>
<span id="cb8-13"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-13" tabindex="-1"></a>      <span class="do">## ith.fold = rows which are in the i-th fold</span></span>
<span id="cb8-14"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-14" tabindex="-1"></a>      ith.fold <span class="ot">=</span> id[(fold[i]<span class="sc">+</span><span class="dv">1</span>)<span class="sc">:</span>fold[i<span class="sc">+</span><span class="dv">1</span>]];   </span>
<span id="cb8-15"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-15" tabindex="-1"></a>      tmp <span class="ot">=</span> <span class="fu">knn</span>(traindata[<span class="sc">-</span>ith.fold,], traindata[ith.fold,], Ytrain[<span class="sc">-</span>ith.fold], <span class="at">k=</span>neighbor_size[j]);</span>
<span id="cb8-16"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-16" tabindex="-1"></a>      cv.error[j]<span class="ot">=</span>cv.error[j] <span class="sc">+</span> <span class="fu">sum</span>(tmp <span class="sc">!=</span> Ytrain[ith.fold])</span>
<span id="cb8-17"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-17" tabindex="-1"></a>    }</span>
<span id="cb8-18"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-18" tabindex="-1"></a></span>
<span id="cb8-19"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-19" tabindex="-1"></a><span class="do">## Find the optimal k value based 5-fold CV</span></span>
<span id="cb8-20"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-20" tabindex="-1"></a>k.optimal <span class="ot">=</span> neighbor_size[<span class="fu">order</span>(cv.error)[<span class="dv">1</span>]]  </span>
<span id="cb8-21"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-21" tabindex="-1"></a></span>
<span id="cb8-22"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-22" tabindex="-1"></a><span class="do">## Error of KNN for the k is chosen by 5-fold CV</span></span>
<span id="cb8-23"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-23" tabindex="-1"></a>Ytrain.pred <span class="ot">=</span> <span class="fu">knn</span>(traindata, traindata, Ytrain, <span class="at">k=</span>k.optimal)</span>
<span id="cb8-24"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-24" tabindex="-1"></a>train.err.knn.CV <span class="ot">=</span> <span class="fu">sum</span>(Ytrain <span class="sc">!=</span> Ytrain.pred)<span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>n)</span>
<span id="cb8-25"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-25" tabindex="-1"></a>Ytest.pred <span class="ot">=</span> <span class="fu">knn</span>(traindata, testdata, Ytrain,<span class="at">k=</span>k.optimal)</span>
<span id="cb8-26"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb8-26" tabindex="-1"></a>test.err.knn.CV <span class="ot">=</span> <span class="fu">sum</span>(Ytest <span class="sc">!=</span> Ytest.pred)<span class="sc">/</span>(<span class="dv">2</span><span class="sc">*</span>N)  </span></code></pre></div>
<br>
<hr>
<p><br></p>
</div>
<div id="least-squares-method" class="section level4 hasAnchor" number="1.5.4.8">
<h4><span class="header-section-number">1.5.4.8</span> Least Squares Method<a href="two-toy-examples-knn-vs.-linear-regression.html#least-squares-method" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><br>
We run a regression of <code>Ytrain</code> vs. the <code>traindata</code>, and we classify the results as follows:
<span class="math display">\[\hat{Y} = \begin{cases} &amp; 1, \text{ if } fitted(Y) &gt; 0.5\\
                          &amp; 0, \text{ if } fitted(Y) \leq 0.5
            \end{cases} \]</span></p>
<p><br></p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb9-1" tabindex="-1"></a><span class="do">## Run a regression using the lm function</span></span>
<span id="cb9-2"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb9-2" tabindex="-1"></a><span class="do">## Ytrain is a factor, so we need to convert it to a numeric vector to run the lm</span></span>
<span id="cb9-3"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb9-3" tabindex="-1"></a>RegModel <span class="ot">=</span> <span class="fu">lm</span>(<span class="fu">as.numeric</span>(Ytrain)<span class="sc">-</span><span class="dv">1</span> <span class="sc">~</span> traindata)</span>
<span id="cb9-4"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb9-4" tabindex="-1"></a></span>
<span id="cb9-5"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb9-5" tabindex="-1"></a><span class="do">## Compute the \hat{Y} for training </span></span>
<span id="cb9-6"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb9-6" tabindex="-1"></a>Ytrain_pred_LS <span class="ot">=</span> <span class="fu">as.numeric</span>(RegModel<span class="sc">$</span>fitted <span class="sc">&gt;</span> <span class="fl">0.5</span>)</span>
<span id="cb9-7"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb9-7" tabindex="-1"></a></span>
<span id="cb9-8"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb9-8" tabindex="-1"></a><span class="do">## Compute the predicted values for testing data and then the \hat{Y}</span></span>
<span id="cb9-9"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb9-9" tabindex="-1"></a>Ytest_pred_LS <span class="ot">=</span> RegModel<span class="sc">$</span>coef[<span class="dv">1</span>] <span class="sc">+</span> RegModel<span class="sc">$</span>coef[<span class="dv">2</span>] <span class="sc">*</span> testdata[,<span class="dv">1</span>] <span class="sc">+</span> RegModel<span class="sc">$</span>coef[<span class="dv">3</span>] <span class="sc">*</span> testdata[,<span class="dv">2</span>]</span>
<span id="cb9-10"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb9-10" tabindex="-1"></a>Ytest_pred_LS <span class="ot">=</span> <span class="fu">as.numeric</span>(Ytest_pred_LS <span class="sc">&gt;</span> <span class="fl">0.5</span> )</span>
<span id="cb9-11"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb9-11" tabindex="-1"></a></span>
<span id="cb9-12"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb9-12" tabindex="-1"></a></span>
<span id="cb9-13"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb9-13" tabindex="-1"></a><span class="do">## Cross-tab for training data and training error</span></span>
<span id="cb9-14"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb9-14" tabindex="-1"></a><span class="fu">table</span>(Ytrain, Ytrain_pred_LS);   </span></code></pre></div>
<pre><code>##       Ytrain_pred_LS
## Ytrain  0  1
##      0 84 16
##      1 15 85</code></pre>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb11-1" tabindex="-1"></a>train.err.LS <span class="ot">=</span> <span class="fu">sum</span>(Ytrain <span class="sc">!=</span>  Ytrain_pred_LS) <span class="sc">/</span> (<span class="dv">2</span><span class="sc">*</span>n);  </span>
<span id="cb11-2"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb11-2" tabindex="-1"></a></span>
<span id="cb11-3"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb11-3" tabindex="-1"></a><span class="do">## Cross-tab for test data and test error</span></span>
<span id="cb11-4"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb11-4" tabindex="-1"></a><span class="fu">table</span>(Ytest, Ytest_pred_LS);     </span></code></pre></div>
<pre><code>##      Ytest_pred_LS
## Ytest    0    1
##     0 7471 2529
##     1 2257 7743</code></pre>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb13-1" tabindex="-1"></a>test.err.LS <span class="ot">=</span> <span class="fu">sum</span>(Ytest <span class="sc">!=</span>  Ytest_pred_LS) <span class="sc">/</span> (<span class="dv">2</span><span class="sc">*</span>N);</span></code></pre></div>
<br>
<hr>
<p><br></p>
</div>
<div id="illustration-of-the-results" class="section level4 hasAnchor" number="1.5.4.9">
<h4><span class="header-section-number">1.5.4.9</span> Illustration of the Results<a href="two-toy-examples-knn-vs.-linear-regression.html#illustration-of-the-results" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><br>
First, we illustrate the classification achieved by each method.</p>
<p><br><br></p>
</div>
<div id="knn-classification-for-two-ks-k-15-and-k-1" class="section level4 hasAnchor" number="1.5.4.10">
<h4><span class="header-section-number">1.5.4.10</span> kNN Classification for two k’s: k = 15 and k = 1<a href="two-toy-examples-knn-vs.-linear-regression.html#knn-classification-for-two-ks-k-15-and-k-1" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><br></p>
<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb14-1"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-1" tabindex="-1"></a><span class="do">## Grid Using kNN Classification: We first define the boundaries for the grid</span></span>
<span id="cb14-2"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-2" tabindex="-1"></a></span>
<span id="cb14-3"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-3" tabindex="-1"></a>x.min <span class="ot">=</span> <span class="fu">round</span>(<span class="fu">min</span>(mytraindata<span class="sc">$</span>X1), <span class="at">digits=</span><span class="dv">1</span>)<span class="sc">-</span><span class="fl">0.1</span></span>
<span id="cb14-4"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-4" tabindex="-1"></a>x.max <span class="ot">=</span> <span class="fu">round</span>(<span class="fu">max</span>(mytraindata<span class="sc">$</span>X1), <span class="at">digits=</span><span class="dv">1</span>)<span class="sc">+</span><span class="fl">0.1</span></span>
<span id="cb14-5"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-5" tabindex="-1"></a>y.min <span class="ot">=</span> <span class="fu">round</span>(<span class="fu">min</span>(mytraindata<span class="sc">$</span>X2), <span class="at">digits=</span><span class="dv">1</span>)<span class="sc">-</span><span class="fl">0.1</span></span>
<span id="cb14-6"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-6" tabindex="-1"></a>y.max <span class="ot">=</span> <span class="fu">round</span>(<span class="fu">max</span>(mytraindata<span class="sc">$</span>X2), <span class="at">digits=</span><span class="dv">1</span>)<span class="sc">+</span><span class="fl">0.1</span></span>
<span id="cb14-7"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-7" tabindex="-1"></a>x.range <span class="ot">=</span> <span class="fu">seq</span>(<span class="at">from=</span>x.min, <span class="at">to=</span>x.max, <span class="at">by=</span><span class="fl">0.1</span>)</span>
<span id="cb14-8"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-8" tabindex="-1"></a>y.range <span class="ot">=</span> <span class="fu">seq</span>(<span class="at">from=</span>y.min, <span class="at">to=</span>y.max, <span class="at">by=</span><span class="fl">0.1</span>)</span>
<span id="cb14-9"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-9" tabindex="-1"></a>x.new <span class="ot">=</span> <span class="fu">expand.grid</span>(x.range, y.range)</span>
<span id="cb14-10"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-10" tabindex="-1"></a><span class="fu">names</span>(x.new) <span class="ot">=</span> <span class="fu">names</span>(mytraindata[,<span class="sc">-</span><span class="dv">3</span>])</span>
<span id="cb14-11"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-11" tabindex="-1"></a></span>
<span id="cb14-12"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-12" tabindex="-1"></a><span class="do">## Basic scatterplot (same as before): </span></span>
<span id="cb14-13"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-13" tabindex="-1"></a><span class="do">##   this is used as a base to add the boundary and shaded areas</span></span>
<span id="cb14-14"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-14" tabindex="-1"></a></span>
<span id="cb14-15"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-15" tabindex="-1"></a>grid.plot <span class="ot">=</span>  <span class="fu">ggplot</span>(mytraindata, <span class="fu">aes</span>(X1, X2)) <span class="sc">+</span></span>
<span id="cb14-16"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-16" tabindex="-1"></a>            <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">colour=</span>Y), <span class="at">size=</span><span class="dv">2</span>) <span class="sc">+</span> </span>
<span id="cb14-17"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-17" tabindex="-1"></a>            <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;orange&quot;</span>, <span class="st">&quot;blue&quot;</span>))   <span class="sc">+</span></span>
<span id="cb14-18"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-18" tabindex="-1"></a>            <span class="fu">geom_point</span>(<span class="at">data=</span><span class="fu">data.frame</span>(<span class="at">X1=</span>mu1[<span class="dv">1</span>], <span class="at">X2=</span>mu1[<span class="dv">2</span>]), <span class="fu">aes</span>(X1, X2), <span class="at">colour=</span><span class="st">&quot;red&quot;</span>, <span class="at">shape=</span><span class="dv">2</span>,<span class="at">size=</span><span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb14-19"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-19" tabindex="-1"></a>            <span class="fu">geom_point</span>(<span class="at">data=</span><span class="fu">data.frame</span>(<span class="at">X1=</span>mu0[<span class="dv">1</span>], <span class="at">X2=</span>mu0[<span class="dv">2</span>]), <span class="fu">aes</span>(X1, X2), <span class="at">colour=</span><span class="st">&quot;green&quot;</span>, <span class="at">shape=</span><span class="dv">2</span>, <span class="at">size=</span><span class="dv">5</span>) <span class="sc">+</span></span>
<span id="cb14-20"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-20" tabindex="-1"></a>            <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;&quot;</span>, <span class="at">y=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb14-21"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-21" tabindex="-1"></a></span>
<span id="cb14-22"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-22" tabindex="-1"></a></span>
<span id="cb14-23"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-23" tabindex="-1"></a><span class="do">## Plot for k=15</span></span>
<span id="cb14-24"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-24" tabindex="-1"></a></span>
<span id="cb14-25"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-25" tabindex="-1"></a>knn.yhat<span class="fl">.15</span> <span class="ot">=</span> <span class="fu">knn</span>(traindata, x.new, Ytrain,  <span class="at">k=</span><span class="dv">15</span>) </span>
<span id="cb14-26"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-26" tabindex="-1"></a>knn.pred<span class="fl">.15</span> <span class="ot">=</span> <span class="fu">ifelse</span>(<span class="fu">as.numeric</span>(knn.yhat<span class="fl">.15</span>)<span class="sc">&gt;</span><span class="fl">0.5</span>,<span class="st">&quot;1&quot;</span>,<span class="st">&quot;0&quot;</span>)</span>
<span id="cb14-27"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-27" tabindex="-1"></a></span>
<span id="cb14-28"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-28" tabindex="-1"></a>knn.plotdata<span class="fl">.15</span> <span class="ot">=</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(x.new, knn.yhat<span class="fl">.15</span>))</span>
<span id="cb14-29"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-29" tabindex="-1"></a></span>
<span id="cb14-30"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-30" tabindex="-1"></a></span>
<span id="cb14-31"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-31" tabindex="-1"></a>knn.plot<span class="fl">.15</span> <span class="ot">=</span> grid.plot <span class="sc">+</span> </span>
<span id="cb14-32"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-32" tabindex="-1"></a>            <span class="fu">theme_bw</span>() <span class="sc">+</span> <span class="co">#remove grey background</span></span>
<span id="cb14-33"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-33" tabindex="-1"></a>            <span class="fu">geom_point</span>(<span class="at">data=</span>knn.plotdata<span class="fl">.15</span>, <span class="fu">aes</span>(<span class="at">x=</span>X1, <span class="at">y=</span>X2, <span class="at">colour=</span>knn.yhat<span class="fl">.15</span>), <span class="at">size=</span><span class="fl">0.05</span>) <span class="sc">+</span></span>
<span id="cb14-34"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-34" tabindex="-1"></a>            <span class="fu">geom_contour</span>(<span class="at">data=</span>knn.plotdata<span class="fl">.15</span>, <span class="fu">aes</span>(<span class="at">x=</span>X1, <span class="at">y=</span>X2, <span class="at">z=</span><span class="fu">as.numeric</span>(knn.yhat<span class="fl">.15</span>)), <span class="at">bins=</span><span class="dv">1</span>, <span class="at">color=</span><span class="st">&quot;black&quot;</span>)<span class="sc">+</span></span>
<span id="cb14-35"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-35" tabindex="-1"></a>            <span class="fu">ggtitle</span>(<span class="st">&quot;kNN Classification for k=15&quot;</span>) </span>
<span id="cb14-36"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-36" tabindex="-1"></a></span>
<span id="cb14-37"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb14-37" tabindex="-1"></a><span class="fu">plot</span>(knn.plot<span class="fl">.15</span>)</span></code></pre></div>
<p><img src="my-598PSL_files/figure-html/unnamed-chunk-20-1.png" width="672" /></p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb15-1" tabindex="-1"></a><span class="do">## Plot for k=1</span></span>
<span id="cb15-2"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb15-2" tabindex="-1"></a></span>
<span id="cb15-3"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb15-3" tabindex="-1"></a>knn.yhat<span class="fl">.1</span> <span class="ot">=</span> <span class="fu">knn</span>(traindata, x.new, Ytrain,  <span class="at">k=</span><span class="dv">1</span>) </span>
<span id="cb15-4"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb15-4" tabindex="-1"></a>knn.pred<span class="fl">.1</span> <span class="ot">=</span> <span class="fu">ifelse</span>(<span class="fu">as.numeric</span>(knn.yhat<span class="fl">.1</span>)<span class="sc">&gt;</span><span class="fl">0.5</span>, <span class="st">&quot;1&quot;</span>, <span class="st">&quot;0&quot;</span>)</span>
<span id="cb15-5"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb15-5" tabindex="-1"></a></span>
<span id="cb15-6"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb15-6" tabindex="-1"></a>knn.plotdata<span class="fl">.1</span> <span class="ot">=</span> <span class="fu">as.data.frame</span>(<span class="fu">cbind</span>(x.new, knn.yhat<span class="fl">.1</span>))</span>
<span id="cb15-7"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb15-7" tabindex="-1"></a></span>
<span id="cb15-8"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb15-8" tabindex="-1"></a></span>
<span id="cb15-9"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb15-9" tabindex="-1"></a>knn.plot<span class="fl">.1</span> <span class="ot">=</span> grid.plot <span class="sc">+</span> </span>
<span id="cb15-10"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb15-10" tabindex="-1"></a>            <span class="fu">theme_bw</span>() <span class="sc">+</span> </span>
<span id="cb15-11"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb15-11" tabindex="-1"></a>            <span class="fu">geom_point</span>(<span class="at">data=</span>knn.plotdata<span class="fl">.1</span>, <span class="fu">aes</span>(<span class="at">x=</span>X1, <span class="at">y=</span>X2, <span class="at">colour=</span>knn.yhat<span class="fl">.1</span>), <span class="at">size=</span><span class="fl">0.05</span>) <span class="sc">+</span></span>
<span id="cb15-12"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb15-12" tabindex="-1"></a>            <span class="fu">geom_contour</span>(<span class="at">data=</span>knn.plotdata<span class="fl">.1</span>, <span class="fu">aes</span>(<span class="at">x=</span>X1, <span class="at">y=</span>X2, <span class="at">z=</span><span class="fu">as.numeric</span>(knn.yhat<span class="fl">.1</span>)), <span class="at">bins=</span><span class="dv">1</span>, <span class="at">color=</span><span class="st">&quot;black&quot;</span>)<span class="sc">+</span></span>
<span id="cb15-13"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb15-13" tabindex="-1"></a>            <span class="fu">ggtitle</span>(<span class="st">&quot;kNN Classification for k=1&quot;</span>) </span>
<span id="cb15-14"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb15-14" tabindex="-1"></a></span>
<span id="cb15-15"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb15-15" tabindex="-1"></a><span class="fu">plot</span>(knn.plot<span class="fl">.1</span>)</span></code></pre></div>
<p><img src="my-598PSL_files/figure-html/unnamed-chunk-20-2.png" width="672" /></p>
<p><br><br></p>
</div>
<div id="regression-classification" class="section level4 hasAnchor" number="1.5.4.11">
<h4><span class="header-section-number">1.5.4.11</span> Regression Classification<a href="two-toy-examples-knn-vs.-linear-regression.html#regression-classification" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><br></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb16-1"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-1" tabindex="-1"></a><span class="do">## Grid Using Linear Model Classification</span></span>
<span id="cb16-2"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-2" tabindex="-1"></a></span>
<span id="cb16-3"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-3" tabindex="-1"></a>RegModel <span class="ot">=</span> <span class="fu">lm</span>(<span class="fu">as.numeric</span>(Ytrain)<span class="sc">-</span><span class="dv">1</span> <span class="sc">~</span> traindata)</span>
<span id="cb16-4"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-4" tabindex="-1"></a>beta.hat <span class="ot">=</span> <span class="fu">coef</span>(RegModel)</span>
<span id="cb16-5"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-5" tabindex="-1"></a></span>
<span id="cb16-6"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-6" tabindex="-1"></a><span class="do">## We use the same ranges for the grid as before</span></span>
<span id="cb16-7"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-7" tabindex="-1"></a></span>
<span id="cb16-8"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-8" tabindex="-1"></a>Reg.yhat <span class="ot">=</span> <span class="fu">as.matrix</span>(<span class="fu">cbind</span>(<span class="fu">rep</span>(<span class="dv">1</span>, <span class="fu">nrow</span>(x.new)), x.new)) <span class="sc">%*%</span> beta.hat   <span class="do">## predicted Y values</span></span>
<span id="cb16-9"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-9" tabindex="-1"></a>Reg.pred <span class="ot">=</span> <span class="fu">ifelse</span>(Reg.yhat <span class="sc">&gt;</span> <span class="fl">0.5</span>, <span class="st">&quot;1&quot;</span>, <span class="st">&quot;0&quot;</span>)   <span class="do">## convert numeric Y to factor</span></span>
<span id="cb16-10"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-10" tabindex="-1"></a></span>
<span id="cb16-11"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-11" tabindex="-1"></a>Reg.plot.data <span class="ot">=</span> <span class="fu">cbind</span>(x.new, Reg.pred)</span>
<span id="cb16-12"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-12" tabindex="-1"></a></span>
<span id="cb16-13"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-13" tabindex="-1"></a><span class="do">## Basic plot is same as before, so now we add the regression boundary and classiffication results</span></span>
<span id="cb16-14"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-14" tabindex="-1"></a>reg.plot <span class="ot">=</span> grid.plot <span class="sc">+</span> </span>
<span id="cb16-15"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-15" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span> </span>
<span id="cb16-16"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-16" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">slope=</span><span class="sc">-</span>beta.hat[<span class="dv">2</span>]<span class="sc">/</span>beta.hat[<span class="dv">3</span>], <span class="at">intercept=</span>(.<span class="dv">5</span><span class="sc">-</span>beta.hat[<span class="dv">1</span>])<span class="sc">/</span>beta.hat[<span class="dv">3</span>],<span class="at">color=</span><span class="st">&quot;black&quot;</span>)  <span class="sc">+</span></span>
<span id="cb16-17"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-17" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">data =</span> Reg.plot.data, <span class="fu">aes</span>(<span class="at">x=</span>X1, <span class="at">y=</span>X2, <span class="at">color=</span>Reg.pred), <span class="at">size=</span><span class="fl">0.05</span>) <span class="sc">+</span></span>
<span id="cb16-18"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-18" tabindex="-1"></a>  <span class="fu">ggtitle</span>(<span class="st">&quot;Regression Classification&quot;</span>)</span>
<span id="cb16-19"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-19" tabindex="-1"></a></span>
<span id="cb16-20"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb16-20" tabindex="-1"></a><span class="fu">plot</span>(reg.plot)</span></code></pre></div>
<p><img src="my-598PSL_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p><br><br></p>
</div>
<div id="plot-the-performance-and-compare-the-two-methods" class="section level4 hasAnchor" number="1.5.4.12">
<h4><span class="header-section-number">1.5.4.12</span> Plot the Performance and Compare the two Methods<a href="two-toy-examples-knn-vs.-linear-regression.html#plot-the-performance-and-compare-the-two-methods" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p><br></p>
<p>Test errors are in <code>magenta</code> and training errors are in <code>blue</code>. The upper <span class="math inline">\(x\)</span>-coordinate indicates the <span class="math inline">\(k\)</span> values, and the lower <span class="math inline">\(x\)</span>-coordinate indicates the degrees-of-freedom of the <span class="math inline">\(k\)</span>NN procedures so that the labels are reciprocally related to <span class="math inline">\(k\)</span>.</p>
<p><br></p>
<p>The training and test errors for linear regression are plotted at <span class="math inline">\(df = 3\)</span> (corresponding to <span class="math inline">\(k = (2n)/3\)</span>), since the linear model has 3 parameters, i.e., 3 dfs.</p>
<p><br></p>
<p>The training and test errors for KNN with <span class="math inline">\(k\)</span> chosen by CV are plotted at the chose <span class="math inline">\(k\)</span> values.</p>
<p><br></p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-1" tabindex="-1"></a><span class="fu">plot</span>(<span class="fu">c</span>(<span class="fl">0.5</span>,m), <span class="fu">range</span>(test.err.LS, train.err.LS, test.err.knn, train.err.knn),</span>
<span id="cb17-2"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-2" tabindex="-1"></a>                  <span class="at">type=</span><span class="st">&quot;n&quot;</span>, <span class="at">xlab=</span><span class="st">&quot;Degrees of Freedom&quot;</span>, <span class="at">ylab=</span><span class="st">&quot;Error&quot;</span>, <span class="at">xaxt=</span><span class="st">&quot;n&quot;</span>)</span>
<span id="cb17-3"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-3" tabindex="-1"></a></span>
<span id="cb17-4"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-4" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">round</span>((<span class="dv">2</span><span class="sc">*</span>n)<span class="sc">/</span>neighbor_size)</span>
<span id="cb17-5"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-5" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at=</span><span class="dv">1</span><span class="sc">:</span>m, <span class="at">labels=</span>df)</span>
<span id="cb17-6"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-6" tabindex="-1"></a><span class="fu">axis</span>(<span class="dv">3</span>, <span class="at">at=</span><span class="dv">1</span><span class="sc">:</span>m, <span class="at">labels=</span>neighbor_size)</span>
<span id="cb17-7"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-7" tabindex="-1"></a></span>
<span id="cb17-8"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-8" tabindex="-1"></a><span class="fu">points</span>(<span class="dv">1</span><span class="sc">:</span>m, test.err.knn, <span class="at">col=</span><span class="st">&quot;magenta&quot;</span>, <span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb17-9"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-9" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span>m, test.err.knn, <span class="at">col=</span><span class="st">&quot;magenta&quot;</span>, <span class="at">lty=</span><span class="dv">1</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb17-10"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-10" tabindex="-1"></a><span class="fu">points</span>(<span class="dv">1</span><span class="sc">:</span>m, train.err.knn, <span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">pch=</span><span class="dv">16</span>)</span>
<span id="cb17-11"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-11" tabindex="-1"></a><span class="fu">lines</span>(<span class="dv">1</span><span class="sc">:</span>m, train.err.knn, <span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">lty=</span><span class="dv">3</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb17-12"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-12" tabindex="-1"></a></span>
<span id="cb17-13"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-13" tabindex="-1"></a><span class="fu">points</span>(<span class="dv">3</span>, train.err.LS, <span class="at">pch=</span><span class="dv">2</span>, <span class="at">cex=</span><span class="dv">2</span>, <span class="at">col=</span><span class="st">&quot;blue&quot;</span>)</span>
<span id="cb17-14"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-14" tabindex="-1"></a><span class="fu">points</span>(<span class="dv">3</span>, test.err.LS, <span class="at">pch=</span><span class="dv">2</span>, <span class="at">cex=</span><span class="dv">2</span>, <span class="at">col=</span><span class="st">&quot;magenta&quot;</span>)</span>
<span id="cb17-15"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-15" tabindex="-1"></a></span>
<span id="cb17-16"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-16" tabindex="-1"></a><span class="fu">points</span>((<span class="dv">1</span><span class="sc">:</span>m)[neighbor_size <span class="sc">==</span> k.optimal], train.err.knn.CV, <span class="at">col=</span><span class="st">&quot;blue&quot;</span>, <span class="at">pch=</span><span class="dv">5</span>, <span class="at">cex=</span><span class="dv">2</span>)</span>
<span id="cb17-17"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-17" tabindex="-1"></a><span class="fu">points</span>((<span class="dv">1</span><span class="sc">:</span>m)[neighbor_size <span class="sc">==</span> k.optimal], test.err.knn.CV, <span class="at">col=</span><span class="st">&quot;magenta&quot;</span>, <span class="at">pch=</span><span class="dv">5</span>, <span class="at">cex=</span><span class="dv">2</span>)</span>
<span id="cb17-18"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-18" tabindex="-1"></a></span>
<span id="cb17-19"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-19" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;bottomleft&quot;</span>, <span class="at">pch =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">5</span>, <span class="dv">16</span>, <span class="dv">16</span>),  <span class="at">col =</span> <span class="fu">c</span>(<span class="st">&quot;blue&quot;</span>, <span class="st">&quot;magenta&quot;</span>), </span>
<span id="cb17-20"><a href="two-toy-examples-knn-vs.-linear-regression.html#cb17-20" tabindex="-1"></a>       <span class="at">legend =</span> <span class="fu">c</span>(<span class="st">&quot;Regression Training Error&quot;</span>, <span class="st">&quot;Regression Testing Error&quot;</span>, <span class="st">&quot;kNN Optimal k Training Error&quot;</span>, <span class="st">&quot;kNN Optimal k Testing Error&quot;</span>, <span class="st">&quot;kNN different k Training Error&quot;</span>, <span class="st">&quot;kNN different k Testing Error&quot;</span>), <span class="at">cex=</span><span class="fl">0.75</span>)</span></code></pre></div>
<p><img src="my-598PSL_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>

</div>
</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="bias-variance-trade-off.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="linear-regression-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans serif",
    "size": 1
  },
  "edit": {
    "link": "https://github.com/USERNAME/REPO/edit/BRANCH/01-intro.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["my-598PSL.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section",
    "toc_depth": 4,
    "toc_float": true,
    "scroll_highlight": true
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
