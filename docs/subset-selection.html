<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.2 Subset Selection | CS 598 Practical Statistical Learning</title>
  <meta name="description" content="Lecture notes for “CS 598 PSL”." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="3.2 Subset Selection | CS 598 Practical Statistical Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for “CS 598 PSL”." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.2 Subset Selection | CS 598 Practical Statistical Learning" />
  
  <meta name="twitter:description" content="Lecture notes for “CS 598 PSL”." />
  

<meta name="author" content="Alexandra Chronopoulou" />


<meta name="date" content="2025-08-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="training-vs.-testing-errors.html"/>
<link rel="next" href="shrinkage-methods.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">CS 598 PSL A. Chronopoulou</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><font color="darkblue"> Course Information </font></a></li>
<li class="chapter" data-level="1" data-path="introduction-to-statistical-learning.html"><a href="introduction-to-statistical-learning.html"><i class="fa fa-check"></i><b>1</b> Introduction to Statistical Learning</a>
<ul>
<li class="chapter" data-level="1.1" data-path="examples-of-statistical-learning-problems.html"><a href="examples-of-statistical-learning-problems.html"><i class="fa fa-check"></i><b>1.1</b> Examples of Statistical Learning Problems</a></li>
<li class="chapter" data-level="1.2" data-path="supervised-learning-framework.html"><a href="supervised-learning-framework.html"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning Framework</a></li>
<li class="chapter" data-level="1.3" data-path="why-is-statistical-learning-is-challenging.html"><a href="why-is-statistical-learning-is-challenging.html"><i class="fa fa-check"></i><b>1.3</b> Why is Statistical Learning is Challenging</a></li>
<li class="chapter" data-level="1.4" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>1.4</b> Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="1.5" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html"><i class="fa fa-check"></i><b>1.5</b> Two Toy Examples: <span class="math inline">\(k\)</span>NN vs. Linear Regression</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>1.5.1</b> <span class="math inline">\(k\)</span>-Nearest Neighbors</a></li>
<li class="chapter" data-level="1.5.2" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html#linear-regression"><i class="fa fa-check"></i><b>1.5.2</b> Linear Regression</a></li>
<li class="chapter" data-level="1.5.3" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html#simulated-binary-classification-example"><i class="fa fa-check"></i><b>1.5.3</b> Simulated Binary Classification Example</a></li>
<li class="chapter" data-level="1.5.4" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html#code-for-the-examples-in-the-lectures"><i class="fa fa-check"></i><b>1.5.4</b> Code for the Examples in the Lectures</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression-models.html"><a href="linear-regression-models.html"><i class="fa fa-check"></i><b>2</b> Linear Regression Models</a>
<ul>
<li class="chapter" data-level="2.1" data-path="multiple-linear-regression-mlr-model.html"><a href="multiple-linear-regression-mlr-model.html"><i class="fa fa-check"></i><b>2.1</b> Multiple Linear Regression (MLR) Model</a></li>
<li class="chapter" data-level="2.2" data-path="mlr-model-fitting.html"><a href="mlr-model-fitting.html"><i class="fa fa-check"></i><b>2.2</b> MLR Model Fitting</a></li>
<li class="chapter" data-level="2.3" data-path="least-squares-normal-equations.html"><a href="least-squares-normal-equations.html"><i class="fa fa-check"></i><b>2.3</b> Least-Squares &amp; Normal Equations</a></li>
<li class="chapter" data-level="2.4" data-path="goodness-of-fit-r-square.html"><a href="goodness-of-fit-r-square.html"><i class="fa fa-check"></i><b>2.4</b> Goodness-Of-Fit: <span class="math inline">\(R\)</span>-Square</a></li>
<li class="chapter" data-level="2.5" data-path="linear-transformations-on-x.html"><a href="linear-transformations-on-x.html"><i class="fa fa-check"></i><b>2.5</b> Linear Transformations on <span class="math inline">\(X\)</span></a></li>
<li class="chapter" data-level="2.6" data-path="rank-deficiency.html"><a href="rank-deficiency.html"><i class="fa fa-check"></i><b>2.6</b> Rank deficiency</a></li>
<li class="chapter" data-level="2.7" data-path="hypothesis-testing-in-mlr.html"><a href="hypothesis-testing-in-mlr.html"><i class="fa fa-check"></i><b>2.7</b> Hypothesis Testing in MLR</a></li>
<li class="chapter" data-level="2.8" data-path="categorical-variables-in-mlr.html"><a href="categorical-variables-in-mlr.html"><i class="fa fa-check"></i><b>2.8</b> Categorical Variables in MLR</a></li>
<li class="chapter" data-level="2.9" data-path="collinearity.html"><a href="collinearity.html"><i class="fa fa-check"></i><b>2.9</b> Collinearity</a></li>
<li class="chapter" data-level="2.10" data-path="model-diagnostics.html"><a href="model-diagnostics.html"><i class="fa fa-check"></i><b>2.10</b> Model Diagnostics</a></li>
<li class="chapter" data-level="2.11" data-path="the-birthweight-data-set-example.html"><a href="the-birthweight-data-set-example.html"><i class="fa fa-check"></i><b>2.11</b> The <code>Birthweight</code> Data Set Example</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="variable-selection-regularization.html"><a href="variable-selection-regularization.html"><i class="fa fa-check"></i><b>3</b> Variable Selection &amp; Regularization</a>
<ul>
<li class="chapter" data-level="3.1" data-path="training-vs.-testing-errors.html"><a href="training-vs.-testing-errors.html"><i class="fa fa-check"></i><b>3.1</b> Training vs. Testing Errors</a></li>
<li class="chapter" data-level="3.2" data-path="subset-selection.html"><a href="subset-selection.html"><i class="fa fa-check"></i><b>3.2</b> Subset Selection</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="subset-selection.html"><a href="subset-selection.html#information-criteria-based-procedures"><i class="fa fa-check"></i><b>3.2.1</b> Information Criteria-based procedures</a></li>
<li class="chapter" data-level="3.2.2" data-path="subset-selection.html"><a href="subset-selection.html#search-algorithms"><i class="fa fa-check"></i><b>3.2.2</b> Search Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html"><i class="fa fa-check"></i><b>3.3</b> Shrinkage Methods</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#ridge-regression"><i class="fa fa-check"></i><b>3.3.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="3.3.2" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#lasso-regression"><i class="fa fa-check"></i><b>3.3.2</b> Lasso Regression</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-student-performance-example.html"><a href="the-student-performance-example.html"><i class="fa fa-check"></i><b>3.4</b> The <code>Student Performance</code> Example</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="nonlinear-regression.html"><a href="nonlinear-regression.html"><i class="fa fa-check"></i><b>4</b> NonLinear Regression</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="nonlinear-regression.html"><a href="nonlinear-regression.html#a-note-on-nonlinearity"><i class="fa fa-check"></i><b>4.0.1</b> A Note on Nonlinearity</a></li>
<li class="chapter" data-level="4.1" data-path="polynomial-regression.html"><a href="polynomial-regression.html"><i class="fa fa-check"></i><b>4.1</b> Polynomial Regression</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="polynomial-regression.html"><a href="polynomial-regression.html#polynomial-basis-functions"><i class="fa fa-check"></i><b>4.1.1</b> Polynomial Basis Functions</a></li>
<li class="chapter" data-level="4.1.2" data-path="polynomial-regression.html"><a href="polynomial-regression.html#polynomial-regression-1"><i class="fa fa-check"></i><b>4.1.2</b> Polynomial Regression</a></li>
<li class="chapter" data-level="4.1.3" data-path="polynomial-regression.html"><a href="polynomial-regression.html#orthogonal-polynomials"><i class="fa fa-check"></i><b>4.1.3</b> Orthogonal Polynomials</a></li>
<li class="chapter" data-level="4.1.4" data-path="polynomial-regression.html"><a href="polynomial-regression.html#piece-wise-polynomials"><i class="fa fa-check"></i><b>4.1.4</b> Piece-wise Polynomials</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="splines-regression.html"><a href="splines-regression.html"><i class="fa fa-check"></i><b>4.2</b> Splines Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="splines-regression.html"><a href="splines-regression.html#examples-of-cubic-splines-basis"><i class="fa fa-check"></i><b>4.2.1</b> Examples of Cubic Splines Basis</a></li>
<li class="chapter" data-level="4.2.2" data-path="splines-regression.html"><a href="splines-regression.html#b-splines-basis-functions-in-r"><i class="fa fa-check"></i><b>4.2.2</b> B-Splines Basis Functions in <code>R</code></a></li>
<li class="chapter" data-level="4.2.3" data-path="splines-regression.html"><a href="splines-regression.html#natural-cubic-splines-ncs"><i class="fa fa-check"></i><b>4.2.3</b> Natural Cubic Splines (NCS)</a></li>
<li class="chapter" data-level="4.2.4" data-path="splines-regression.html"><a href="splines-regression.html#regression-splines"><i class="fa fa-check"></i><b>4.2.4</b> Regression Splines</a></li>
<li class="chapter" data-level="4.2.5" data-path="splines-regression.html"><a href="splines-regression.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>4.2.5</b> K-Fold Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="smoothing-splines.html"><a href="smoothing-splines.html"><i class="fa fa-check"></i><b>4.3</b> Smoothing Splines</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="smoothing-splines.html"><a href="smoothing-splines.html#the-roughness-penalty-approach"><i class="fa fa-check"></i><b>4.3.1</b> The Roughness Penalty Approach</a></li>
<li class="chapter" data-level="4.3.2" data-path="smoothing-splines.html"><a href="smoothing-splines.html#proof"><i class="fa fa-check"></i><b>4.3.2</b> Proof</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="fitting-smoothing-splines.html"><a href="fitting-smoothing-splines.html"><i class="fa fa-check"></i><b>4.4</b> Fitting Smoothing Splines</a></li>
<li class="chapter" data-level="4.5" data-path="the-birthrates-example-in-r.html"><a href="the-birthrates-example-in-r.html"><i class="fa fa-check"></i><b>4.5</b> The <code>Birthrates</code> Example in R</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">CS 598 Practical Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="subset-selection" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Subset Selection<a href="subset-selection.html#subset-selection" class="anchor-section" aria-label="Anchor link to header"></a></h2>
The idea behind <em>subset selection</em> is to score each model according to an information criterion and then use a search algorithm to find the optimal model. In this way, we take into account
<div class="motivationbox">
<p><span class="math display">\[\text{Training Error + Complexity Penalty}\]</span></p>
</div>
<p>In the context of linear regression models, the <em>complexity of a model increases with the number of predictor variables</em> (i.e., <span class="math inline">\(p\)</span>).</p>
<ul>
<li><p><strong>Training Error</strong>: an increasing function of <span class="math inline">\(RSS\)</span>.</p></li>
<li><p><strong>Complexity Penalty</strong>: an increasing function of <span class="math inline">\(p\)</span>.</p></li>
</ul>
<p><br></p>
<blockquote>
<p><em>Why don’t we just use <span class="math inline">\(R^2\)</span> or <span class="math inline">\(RSS\)</span>?</em> The main reason is that <span class="math inline">\(R^2\)</span> always increases when we introduce variables in the model, while <span class="math inline">\(RSS\)</span> always reduces. Therefore, <span class="math inline">\(R^2\)</span> and <span class="math inline">\(RSS\)</span> do not penalize for introducing unnecessary variables in the model.</p>
</blockquote>
<p><br><br></p>
<div id="information-criteria-based-procedures" class="section level3 hasAnchor" number="3.2.1">
<h3><span class="header-section-number">3.2.1</span> Information Criteria-based procedures<a href="subset-selection.html#information-criteria-based-procedures" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><font color=blue><strong>Akaike Information Criterion &amp; Bayesian Information Criterion</strong> </font></p>
<p><span class="math inline">\(AIC\)</span> and <span class="math inline">\(BIC\)</span> are defined as</p>
<span class="math display">\[\begin{align*}
AIC &amp;= -2\cdot loglik  + 2\; p \\
BIC &amp;= -2\cdot loglik  + \log(n)\; p
\end{align*}\]</span>
where <span class="math inline">\(p\)</span> is the number of predictors included in model under consideration. <span class="math inline">\(loglik\)</span> denotes the log-likelihood of the model under consideration. For the <font color="darkblue">normal-error linear regression model</font>, the first term computes:
<span class="math display">\[-2 \cdot  loglik = n\log \frac{RSS}{n} \]</span>
which means that
<div class="motivationbox">
<p><span class="math display">\[\begin{align*}
AIC &amp;= n\log \frac{RSS}{n} + 2\; p\\
BIC &amp;= n\log \frac{RSS}{n} + \log(n)\; p
\end{align*}\]</span></p>
</div>
<p>The <strong>lower</strong> the AIC/BIC the <strong>better</strong>. Note that when <span class="math inline">\(n\)</span> is large, <em>adding an additional predictor costs a lot more in BIC than AIC</em>. So, AIC tends to pick a bigger model than BIC.
<br><br></p>
<p><font color=blue><strong>Adjusted-<span class="math inline">\(R^2\)</span> for model with <span class="math inline">\(p\)</span> predictors</strong></font></p>
<div class="motivationbox">
<p><span class="math display">\[\begin{align*}
R^2_a &amp; = 1-\frac{RSS/(n-p-1)}{TSS/(n-1)}\\
&amp; = 1- (1-R^2)\Bigl(\frac{n-1}{n-p-1}\Bigr)\\
&amp; = {\color{blue}{1-\frac{\hat{\sigma}^2}{\hat{\sigma}^2_0}}}
\end{align*}\]</span>
where <span class="math inline">\(\hat{\sigma}\)</span> is the estimated error variance for the current fitted model, and <span class="math inline">\(\hat{\sigma}^2_0\)</span> is the estimated error variance for the full fitted model.</p>
</div>
<p>The <strong>higher</strong> the <span class="math inline">\(R^2_a\)</span> the <strong>better</strong>.</p>
<p><br><br></p>
<p><font color=blue><strong>Mallow’s <span class="math inline">\(C_p\)</span></strong></font></p>
<div class="motivationbox">
<p><span class="math display">\[C_p = \frac{RSS_{\mathbf{p}}}{\hat{\sigma}_0^2} +2p-n\]</span></p>
</div>
<p>where <span class="math inline">\(\hat{\sigma}_0^2\)</span> is the estimate of the error variance for the full model.
Mallow’s <span class="math inline">\(C_p\)</span> behaves very similar to AIC and the <strong>lower</strong> the <span class="math inline">\(C_p\)</span> the <strong>better</strong>.</p>
<p><br><br></p>
<div class="examplebox">
<p><strong>Illustration of Complexity Criteria vs. <span class="math inline">\(p\)</span></strong><br></p>
<p>As an illustration, the complexity penalties <span class="math inline">\(R^2_{\text{adjusted}}\)</span>, Mallow’s <span class="math inline">\(C_p\)</span>, <span class="math inline">\(AIC\)</span>, and <span class="math inline">\(BIC\)</span> are plotted as a function of <span class="math inline">\(p\)</span> (the number of parameters in the model) for the <code>student-grades</code> data set (the example in the last section):
<br></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-66"></span>
<img src="images/week3/criteria.png" alt="Subset selection criteria vs. p." width="95%" />
<p class="caption">
Figure 3.1: Subset selection criteria vs. p.
</p>
</div>
</div>
<br>
<hr>
<p><br></p>
</div>
<div id="search-algorithms" class="section level3 hasAnchor" number="3.2.2">
<h3><span class="header-section-number">3.2.2</span> Search Algorithms<a href="subset-selection.html#search-algorithms" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the beginning of this section, we discussed that to select the optimal model, we need to penalize the models using the preferred penalty. If we have (in total) <span class="math inline">\(p\)</span> available predictors, then there are <span class="math inline">\(2^p\)</span> potential models. Ideally, we would like to search and score all of these models. If <span class="math inline">\(p\)</span> is large, then we need to employ systematic algorithms that search and score potential models.<br></p>
<p><br></p>
<p><font color="darkblue"><strong>Level-wise search algorithms</strong></font>: These are algorithms return the global optimal solution among <em>all possible models</em> and work only for less than <em>40</em> variables.
<br></p>
<p>The idea is that they finds <span class="math inline">\(m\)</span> different models (default <span class="math inline">\(m\)</span> in <code>R</code> is 8) of up to size <span class="math inline">\(p\)</span> with the smallest <span class="math inline">\(RSS\)</span> among all the models of the same size. Then, they evaluate the score on the <span class="math inline">\(p\)</span> models and report the optimal one. It is important to note that the algorithm does not need to visit every model. For example, if we know that <span class="math inline">\(RSS(X1, X2) &lt; RSS(X3, X4, X5, X6)\)</span>, then it is not necessary to search any size 2 or 3 <em>sub-models</em> of set <span class="math inline">\((X3, X4, X5, X6)\)</span> meaning that these models may be leaped over.</p>
<p><br><br></p>
<p><font color="darkblue"><strong>Greedy algorithms</strong></font>: These algorithms add and/or remove variables based on the score given by a criterion such as AIC/BIC. They can move :</p>
<ul>
<li><p><font color=blue><em>Backwards</em></font>: start with the full model and sequentially delete predictors until the score does not improve.</p></li>
<li><p><font color=blue><em>Forward</em></font>: start with the null model and sequentially add predictors until the score does not improve.</p></li>
<li><p><font color=blue><em>Stepwise</em></font>: consider both deleting and adding one predictor at each stage.</p></li>
</ul>
<p>These algorithms are computationally efficient, but they only return a <em>locally optimal</em> solution which usually good enough in practice.
<br>
<br></p>
<div id="some-considerations" class="section level4 unnumbered hasAnchor">
<h4>Some considerations<a href="subset-selection.html#some-considerations" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>In some cases, specially when <span class="math inline">\(p\gg n\)</span> starting with the full model and using the stepwise procedure is not feasible. Therefore, we need to screen some of the variables and remove them, before employing the search algorithms. A common variable screening approach is the following:</p>
<ul>
<li><p>Pick a <em>smaller initial model</em> as a starting point by <strong>ranking</strong> the <span class="math inline">\(p\)</span> predictors by the absolute value of their (marginal) correlation with <span class="math inline">\(Y\)</span>.</p></li>
<li><p>Keep the <strong>top</strong> <span class="math inline">\(K\)</span> predictors (e.g., <span class="math inline">\(K = n/3\)</span>).</p></li>
<li><p>Use the stepwise procedure so that you can add removed variables back to the model.</p></li>
</ul>
<br>
<hr>
<p><br></p>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="training-vs.-testing-errors.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="shrinkage-methods.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans serif",
    "size": 1
  },
  "edit": {
    "link": "https://github.com/USERNAME/REPO/edit/BRANCH/03-variableselection.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["my-598PSL.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section",
    "toc_depth": 4,
    "toc_float": true,
    "scroll_highlight": true
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
