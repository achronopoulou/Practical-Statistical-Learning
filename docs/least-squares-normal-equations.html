<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.3 Least-Squares &amp; Normal Equations | CS 598 Practical Statistical Learning</title>
  <meta name="description" content="Lecture notes for “CS 598 PSL”." />
  <meta name="generator" content="bookdown 0.43 and GitBook 2.6.7" />

  <meta property="og:title" content="2.3 Least-Squares &amp; Normal Equations | CS 598 Practical Statistical Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Lecture notes for “CS 598 PSL”." />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.3 Least-Squares &amp; Normal Equations | CS 598 Practical Statistical Learning" />
  
  <meta name="twitter:description" content="Lecture notes for “CS 598 PSL”." />
  

<meta name="author" content="Alexandra Chronopoulou" />


<meta name="date" content="2025-08-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mlr-model-fitting.html"/>
<link rel="next" href="goodness-of-fit-r-square.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">CS 598 PSL A. Chronopoulou</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><font color="darkblue"> Course Information </font></a></li>
<li class="chapter" data-level="1" data-path="introduction-to-statistical-learning.html"><a href="introduction-to-statistical-learning.html"><i class="fa fa-check"></i><b>1</b> Introduction to Statistical Learning</a>
<ul>
<li class="chapter" data-level="1.1" data-path="examples-of-statistical-learning-problems.html"><a href="examples-of-statistical-learning-problems.html"><i class="fa fa-check"></i><b>1.1</b> Examples of Statistical Learning Problems</a></li>
<li class="chapter" data-level="1.2" data-path="supervised-learning-framework.html"><a href="supervised-learning-framework.html"><i class="fa fa-check"></i><b>1.2</b> Supervised Learning Framework</a></li>
<li class="chapter" data-level="1.3" data-path="why-is-statistical-learning-is-challenging.html"><a href="why-is-statistical-learning-is-challenging.html"><i class="fa fa-check"></i><b>1.3</b> Why is Statistical Learning is Challenging</a></li>
<li class="chapter" data-level="1.4" data-path="bias-variance-trade-off.html"><a href="bias-variance-trade-off.html"><i class="fa fa-check"></i><b>1.4</b> Bias-Variance Trade-Off</a></li>
<li class="chapter" data-level="1.5" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html"><i class="fa fa-check"></i><b>1.5</b> Two Toy Examples: <span class="math inline">\(k\)</span>NN vs. Linear Regression</a>
<ul>
<li class="chapter" data-level="1.5.1" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html#k-nearest-neighbors"><i class="fa fa-check"></i><b>1.5.1</b> <span class="math inline">\(k\)</span>-Nearest Neighbors</a></li>
<li class="chapter" data-level="1.5.2" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html#linear-regression"><i class="fa fa-check"></i><b>1.5.2</b> Linear Regression</a></li>
<li class="chapter" data-level="1.5.3" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html#simulated-binary-classification-example"><i class="fa fa-check"></i><b>1.5.3</b> Simulated Binary Classification Example</a></li>
<li class="chapter" data-level="1.5.4" data-path="two-toy-examples-knn-vs.-linear-regression.html"><a href="two-toy-examples-knn-vs.-linear-regression.html#code-for-the-examples-in-the-lectures"><i class="fa fa-check"></i><b>1.5.4</b> Code for the Examples in the Lectures</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linear-regression-models.html"><a href="linear-regression-models.html"><i class="fa fa-check"></i><b>2</b> Linear Regression Models</a>
<ul>
<li class="chapter" data-level="2.1" data-path="multiple-linear-regression-mlr-model.html"><a href="multiple-linear-regression-mlr-model.html"><i class="fa fa-check"></i><b>2.1</b> Multiple Linear Regression (MLR) Model</a></li>
<li class="chapter" data-level="2.2" data-path="mlr-model-fitting.html"><a href="mlr-model-fitting.html"><i class="fa fa-check"></i><b>2.2</b> MLR Model Fitting</a></li>
<li class="chapter" data-level="2.3" data-path="least-squares-normal-equations.html"><a href="least-squares-normal-equations.html"><i class="fa fa-check"></i><b>2.3</b> Least-Squares &amp; Normal Equations</a></li>
<li class="chapter" data-level="2.4" data-path="goodness-of-fit-r-square.html"><a href="goodness-of-fit-r-square.html"><i class="fa fa-check"></i><b>2.4</b> Goodness-Of-Fit: <span class="math inline">\(R\)</span>-Square</a></li>
<li class="chapter" data-level="2.5" data-path="linear-transformations-on-x.html"><a href="linear-transformations-on-x.html"><i class="fa fa-check"></i><b>2.5</b> Linear Transformations on <span class="math inline">\(X\)</span></a></li>
<li class="chapter" data-level="2.6" data-path="rank-deficiency.html"><a href="rank-deficiency.html"><i class="fa fa-check"></i><b>2.6</b> Rank deficiency</a></li>
<li class="chapter" data-level="2.7" data-path="hypothesis-testing-in-mlr.html"><a href="hypothesis-testing-in-mlr.html"><i class="fa fa-check"></i><b>2.7</b> Hypothesis Testing in MLR</a></li>
<li class="chapter" data-level="2.8" data-path="categorical-variables-in-mlr.html"><a href="categorical-variables-in-mlr.html"><i class="fa fa-check"></i><b>2.8</b> Categorical Variables in MLR</a></li>
<li class="chapter" data-level="2.9" data-path="collinearity.html"><a href="collinearity.html"><i class="fa fa-check"></i><b>2.9</b> Collinearity</a></li>
<li class="chapter" data-level="2.10" data-path="model-diagnostics.html"><a href="model-diagnostics.html"><i class="fa fa-check"></i><b>2.10</b> Model Diagnostics</a></li>
<li class="chapter" data-level="2.11" data-path="the-birthweight-data-set-example.html"><a href="the-birthweight-data-set-example.html"><i class="fa fa-check"></i><b>2.11</b> The <code>Birthweight</code> Data Set Example</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="variable-selection-regularization.html"><a href="variable-selection-regularization.html"><i class="fa fa-check"></i><b>3</b> Variable Selection &amp; Regularization</a>
<ul>
<li class="chapter" data-level="3.1" data-path="training-vs.-testing-errors.html"><a href="training-vs.-testing-errors.html"><i class="fa fa-check"></i><b>3.1</b> Training vs. Testing Errors</a></li>
<li class="chapter" data-level="3.2" data-path="subset-selection.html"><a href="subset-selection.html"><i class="fa fa-check"></i><b>3.2</b> Subset Selection</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="subset-selection.html"><a href="subset-selection.html#information-criteria-based-procedures"><i class="fa fa-check"></i><b>3.2.1</b> Information Criteria-based procedures</a></li>
<li class="chapter" data-level="3.2.2" data-path="subset-selection.html"><a href="subset-selection.html#search-algorithms"><i class="fa fa-check"></i><b>3.2.2</b> Search Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html"><i class="fa fa-check"></i><b>3.3</b> Shrinkage Methods</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#ridge-regression"><i class="fa fa-check"></i><b>3.3.1</b> Ridge Regression</a></li>
<li class="chapter" data-level="3.3.2" data-path="shrinkage-methods.html"><a href="shrinkage-methods.html#lasso-regression"><i class="fa fa-check"></i><b>3.3.2</b> Lasso Regression</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="the-student-performance-example.html"><a href="the-student-performance-example.html"><i class="fa fa-check"></i><b>3.4</b> The <code>Student Performance</code> Example</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="nonlinear-regression.html"><a href="nonlinear-regression.html"><i class="fa fa-check"></i><b>4</b> NonLinear Regression</a>
<ul>
<li class="chapter" data-level="4.0.1" data-path="nonlinear-regression.html"><a href="nonlinear-regression.html#a-note-on-nonlinearity"><i class="fa fa-check"></i><b>4.0.1</b> A Note on Nonlinearity</a></li>
<li class="chapter" data-level="4.1" data-path="polynomial-regression.html"><a href="polynomial-regression.html"><i class="fa fa-check"></i><b>4.1</b> Polynomial Regression</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="polynomial-regression.html"><a href="polynomial-regression.html#polynomial-basis-functions"><i class="fa fa-check"></i><b>4.1.1</b> Polynomial Basis Functions</a></li>
<li class="chapter" data-level="4.1.2" data-path="polynomial-regression.html"><a href="polynomial-regression.html#polynomial-regression-1"><i class="fa fa-check"></i><b>4.1.2</b> Polynomial Regression</a></li>
<li class="chapter" data-level="4.1.3" data-path="polynomial-regression.html"><a href="polynomial-regression.html#orthogonal-polynomials"><i class="fa fa-check"></i><b>4.1.3</b> Orthogonal Polynomials</a></li>
<li class="chapter" data-level="4.1.4" data-path="polynomial-regression.html"><a href="polynomial-regression.html#piece-wise-polynomials"><i class="fa fa-check"></i><b>4.1.4</b> Piece-wise Polynomials</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="splines-regression.html"><a href="splines-regression.html"><i class="fa fa-check"></i><b>4.2</b> Splines Regression</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="splines-regression.html"><a href="splines-regression.html#examples-of-cubic-splines-basis"><i class="fa fa-check"></i><b>4.2.1</b> Examples of Cubic Splines Basis</a></li>
<li class="chapter" data-level="4.2.2" data-path="splines-regression.html"><a href="splines-regression.html#b-splines-basis-functions-in-r"><i class="fa fa-check"></i><b>4.2.2</b> B-Splines Basis Functions in <code>R</code></a></li>
<li class="chapter" data-level="4.2.3" data-path="splines-regression.html"><a href="splines-regression.html#natural-cubic-splines-ncs"><i class="fa fa-check"></i><b>4.2.3</b> Natural Cubic Splines (NCS)</a></li>
<li class="chapter" data-level="4.2.4" data-path="splines-regression.html"><a href="splines-regression.html#regression-splines"><i class="fa fa-check"></i><b>4.2.4</b> Regression Splines</a></li>
<li class="chapter" data-level="4.2.5" data-path="splines-regression.html"><a href="splines-regression.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>4.2.5</b> K-Fold Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="smoothing-splines.html"><a href="smoothing-splines.html"><i class="fa fa-check"></i><b>4.3</b> Smoothing Splines</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="smoothing-splines.html"><a href="smoothing-splines.html#the-roughness-penalty-approach"><i class="fa fa-check"></i><b>4.3.1</b> The Roughness Penalty Approach</a></li>
<li class="chapter" data-level="4.3.2" data-path="smoothing-splines.html"><a href="smoothing-splines.html#proof"><i class="fa fa-check"></i><b>4.3.2</b> Proof</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="fitting-smoothing-splines.html"><a href="fitting-smoothing-splines.html"><i class="fa fa-check"></i><b>4.4</b> Fitting Smoothing Splines</a></li>
<li class="chapter" data-level="4.5" data-path="the-birthrates-example-in-r.html"><a href="the-birthrates-example-in-r.html"><i class="fa fa-check"></i><b>4.5</b> The <code>Birthrates</code> Example in R</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">CS 598 Practical Statistical Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="least-squares-normal-equations" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Least-Squares &amp; Normal Equations<a href="least-squares-normal-equations.html#least-squares-normal-equations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, we focus on the system of linear equations that we solve to obtain the least squares estimators for <span class="math inline">\(\beta\)</span>. Recall, that we want to find a a vector <span class="math inline">\(\hat{\beta}\)</span> that minimizes:
<span class="math display">\[\min_{\beta} ||\mathbf{y} - \mathbf{X} \beta||^2\]</span>
<em>Any vector that provides a minimum value for this expression is called a least-squares solution</em>.</p>
<p>The set of all least squares solutions is precisely the <strong>set of solutions</strong> to
<span class="math display">\[(\mathbf{X}^T \mathbf{X}) \beta = \mathbf{X}^T \mathbf{y}\]</span>
There is a <em>unique</em> solution <strong>if and only if</strong> <span class="math inline">\(rank(\mathbf{X}) = p+1\)</span> in which case <span class="math inline">\(\bigl(\mathbf{X}^T \mathbf{X}\bigr)\)</span> is invertible.</p>
<p><br></p>
<div id="system-of-linear-equations" class="section level3 unlisted unnumbered hasAnchor">
<h3>System of Linear Equations<a href="least-squares-normal-equations.html#system-of-linear-equations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="linear-algebra-review" class="section level5 unnumbered hasAnchor">
<h5>Linear Algebra Review<a href="least-squares-normal-equations.html#linear-algebra-review" class="anchor-section" aria-label="Anchor link to header"></a></h5>
<p>Let us review a few facts from linear algebra related to solutions to a system of linear equations. For simplicity in the notation, we use the <em>generic</em> system of equations:
<span class="math display">\[\mathbf{A} z = c\]</span>
where
<span class="math display">\[ \mathbf{A}=\left(\begin{array}{cccc}
    a_{11} &amp; a_{12} &amp; \cdots &amp; a_{1k} \\
    a_{21} &amp; a_{22} &amp; \cdots &amp; a_{2k}   \\
      \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
  a_{m1} &amp; a_{m2} &amp; \cdots &amp; a_{mk}  \\
    \end{array} \right),\quad
    z =\left(\begin{array}{c}
    z_{1} \\
    z_{2}   \\
   \vdots\\
    z_{m}  \\
    \end{array} \right),\quad
    c =\left(\begin{array}{c}
    c_{1} \\
    c_{2}   \\
   \vdots\\
    c_{m}  \\
    \end{array} \right)
    \]</span></p>
<p>Denote by <span class="math inline">\(\mathbf{A}_{i} = \bigl( a_{1i}, a_{2i}, \ldots, a_{mi}\bigr)^T\)</span> the vector representing the <em>columns</em> of <span class="math inline">\(\mathbf{A}\)</span>. Therefore, <span class="math inline">\(\mathcal{C}(\mathbf{A})\)</span> is the space generated by the columns of <span class="math inline">\(\mathbf{A}\)</span>, or in other words the <span class="math inline">\(span \bigl(\mathbf{A}_1, \mathbf{A}_2, \ldots, \mathbf{A}_{k}\bigr)\)</span>.</p>
<div class="learningbox">
<p><strong>Definition</strong></p>
<p>The <strong>span</strong> of a collection of vectors <span class="math inline">\(\bigl(\mathbf{A}_1, \mathbf{A}_2, \ldots, \mathbf{A}_{k}\bigr)\)</span> is the set of all linear combinations of these vectors:
<span class="math display">\[\begin{align*}
&amp;span \bigl(\mathbf{A}_1, \mathbf{A}_2, \ldots, \mathbf{A}_{k}\bigr) \\
&amp;= \Bigl\{ d_1 \mathbf{A}_1 + d_2 \mathbf{A}_2 + \ldots + d_k \mathbf{A}_{k}, \text{ for any constants } d_1, \ldots, d_k \in \mathbb{R}
\Bigr\}
\end{align*}\]</span></p>
</div>
The column space of <span class="math inline">\(\mathbf{A}\)</span>, <span class="math inline">\(\mathcal{C}(\mathbf{A})\)</span>, is a <strong>subspace</strong> in <span class="math inline">\(\mathbb{R}^n\)</span>. A linear subspace of <span class="math inline">\(\mathbb{R}^n\)</span> is thought of as a “flat” surface within <span class="math inline">\(\mathbb{R}^n\)</span>, and it is a <font color="darkblue">collection of vectors that is closed under linear combinations</font>. Illustration of subspaces of <span class="math inline">\(\mathbb{R}^n\)</span> are shown below:
<center>
<img src="images/week2/subspace1.png" style="width:35.0%" /> <img src="images/week2/subspace2.png" style="width:35.0%" />
</center>
<p><br></p>
</div>
<div id="solving-the-system-of-equations-mathbfazc" class="section level4 unnumbered hasAnchor">
<h4>Solving the System of Equations <span class="math inline">\(\mathbf{A}z=c\)</span><a href="least-squares-normal-equations.html#solving-the-system-of-equations-mathbfazc" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<p>For a system of equations, <span class="math inline">\(\mathbf{A} z = c\)</span>, to have a solution, <span class="math inline">\(c\)</span> must be a linear combination of the columns of <span class="math inline">\(\mathbf{A}\)</span>, i.e. <span class="math inline">\(c \in \mathcal{C}(\mathbf{A})\)</span>. This is simply obtained by the definition of <em>matrix multiplication</em> and <em>equality</em>:
<span class="math display">\[\mathbf{A} z = c \, \Leftrightarrow \, c = z_1 \mathbf{A}_1 + \ldots + z_k \mathbf{A}_k\]</span></p>
<p>When <span class="math inline">\(c \notin \mathcal{C}(\mathbf{A})\)</span>, we need to <strong>find <span class="math inline">\(\hat{c}\)</span> living in <span class="math inline">\(\mathcal{C}(\mathbf{A})\)</span> that is closest to <span class="math inline">\(c\)</span></strong>. If this is the case, <span class="math inline">\(\mathbf{A} z = \hat{c}\)</span> has a unique solution, and <span class="math inline">\(\hat{c}\)</span> comes as close to the original data as possible.
To do so, we need to <strong>project</strong> <span class="math inline">\(c\)</span> <strong>orthogonally</strong> onto <span class="math inline">\(\mathcal{C}(\mathbf{A})\)</span>, by multiplying both sides by <span class="math inline">\(\mathbf{A}^T\)</span>:</p>
<p><img style="float: right;" src="images/week2/LSvec.png" width="275" height="275"></p>
<p><span class="math display">\[\mathbf{A}^T\mathbf{A} z = \mathbf{A}^T c\]</span>
* These are the normal equations.</p>
<p>In the sketch to the right, <span class="math inline">\(\mathcal{C}(\mathbf{A})\)</span>, the space spanned by the columns of <span class="math inline">\(\mathbf{A}\)</span>, is a <em>flat</em> surface, and <span class="math inline">\(c\)</span> is a point that exists off of that flat surface. The <strong>shortest distance</strong> from the point <span class="math inline">\(c\)</span> to the plane <span class="math inline">\(\mathcal{C}(\mathbf{A})\)</span> is the one <em>orthogonal</em> to the plane.</p>
<p>The <em>normal equations</em> essentially help us find the closest point to <span class="math inline">\(c\)</span> that belongs <span class="math inline">\(\mathcal{C}\bigl(\mathbf{A}\Bigr)\)</span> by means of an orthogonal projection.</p>
<p><br></p>
</div>
</div>
<div id="geometric-representation-of-ls" class="section level3 unlisted unnumbered hasAnchor">
<h3>Geometric Representation of LS<a href="least-squares-normal-equations.html#geometric-representation-of-ls" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Taking the previous discussion to the linear regression framework:
<img style="float: right;" src="images/week2/regplanenew.png" width="300" height="275"></p>
<ul>
<li><p><font color="darkblue"><span class="math inline">\(\mathcal{C}(\mathbf{X})\)</span> is the space that is <em>spanned</em> by the predictors <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_p\)</span></font>, or in other words the columns of the design matrix. It is a <em>flat</em> subspace of <span class="math inline">\(\mathbb{R}^n\)</span>.</p></li>
<li><p>The response <span class="math inline">\(\mathbf{y}\)</span> is a vector <strong>off</strong> the subspace, and the fitted value <span class="math inline">\(\hat{\mathbf{y}}\)</span> is the <font color="darkblue">orthogonal projection of <span class="math inline">\(\mathbf{y}\)</span> onto <span class="math inline">\(\mathcal{C}(\mathbf{X})\)</span></font>.</p></li>
<li><p>The residuals, on the other hand, <span class="math inline">\(\mathbf{r} = \mathbf{y} - \hat{\mathbf{y}}\)</span> are <strong>orthogonal</strong> to <span class="math inline">\(\hat{\mathbf{y}}\)</span> and to <span class="math inline">\(\mathcal{C}\bigl( \mathbf{X} \bigr)\)</span>.</p></li>
</ul>
<p><br></p>
<p>Essentially, what the least-squares method does is that it decomposes the data vector <span class="math inline">\(\mathbf{y}\)</span> into two orthogonal components:
<font color="darkblue">
<span class="math display">\[\mathbf{y}_{n\times 1} = \hat{\mathbf{y}}_{n\times 1} + \mathbf{r}_{n\times 1}\]</span>
</font></p>
<p><br></p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mlr-model-fitting.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="goodness-of-fit-r-square.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
  "sharing": {
    "github": false,
    "facebook": true,
    "twitter": true,
    "linkedin": false,
    "weibo": false,
    "instapaper": false,
    "vk": false,
    "whatsapp": false,
    "all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
  },
  "fontsettings": {
    "theme": "white",
    "family": "sans serif",
    "size": 1
  },
  "edit": {
    "link": "https://github.com/USERNAME/REPO/edit/BRANCH/02-regression.Rmd",
    "text": "Edit"
  },
  "history": {
    "link": null,
    "text": null
  },
  "view": {
    "link": null,
    "text": null
  },
  "download": ["my-598PSL.epub"],
  "search": {
    "engine": "fuse",
    "options": null
  },
  "toc": {
    "collapse": "section",
    "toc_depth": 4,
    "toc_float": true,
    "scroll_highlight": true
  }
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
